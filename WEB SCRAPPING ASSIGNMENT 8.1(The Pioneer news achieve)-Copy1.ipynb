{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4de37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\programdata\\anaconda2\\lib\\site-packages (3.141.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda2\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda2\\lib\\site-packages (from selenium) (1.26.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338468b3",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9253bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "409b8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec513e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "date1=[]\n",
    "author1=[]\n",
    "vertical1=[]\n",
    "headline1=[]\n",
    "description1=[]\n",
    "\n",
    "# scrapping details 1 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    try:\n",
    "        date1.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        date1.append('-')\n",
    "        \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "    try:\n",
    "        author1.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        author1.append('-')\n",
    "    \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    try:\n",
    "        vertical1.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        vertical1.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "    try:\n",
    "        headline1.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        headline1.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    try:\n",
    "        description1.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        description1.append('-')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc4f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame({})\n",
    "df1['Date']=date1[:1]\n",
    "df1['Author']=author1[:1]\n",
    "df1['Vertical']=vertical1[:1]\n",
    "df1['Healines']=headline1[:1]\n",
    "df1['Description']=description1[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ce39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea1ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date2=[]\n",
    "author2=[]\n",
    "vertical2=[]\n",
    "headline2=[]\n",
    "description2=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    try:\n",
    "        date2.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        date2.append('-')\n",
    "        \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "    try:\n",
    "        author2.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        author2.append('-')\n",
    "    \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    try:\n",
    "        vertical2.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        vertical2.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "    try:\n",
    "        headline2.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        headline2.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    try:\n",
    "        description2.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        description2.append('-')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd2f9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame({})\n",
    "df2['Date']=date2[:1]\n",
    "df2['Author']=author2[:1]\n",
    "df2['Vertical']=vertical2[:1]\n",
    "df2['Healines']=headline2[:1]\n",
    "df2['Description']=description2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcad136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef005c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "date3=[]\n",
    "author3=[]\n",
    "vertical3=[]\n",
    "headline3=[]\n",
    "description3=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    try:\n",
    "        date3.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        date3.append('-')\n",
    "        \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "    try:\n",
    "        author3.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        author3.append('-')\n",
    "    \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    try:\n",
    "        vertical3.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        vertical3.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "    try:\n",
    "        headline3.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        headline3.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    try:\n",
    "        description3.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        description3.append('-')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "341ddef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.DataFrame({})\n",
    "df3['Date']=date3[:1]\n",
    "df3['Author']=author3[:1]\n",
    "df3['Vertical']=vertical3[:1]\n",
    "df3['Healines']=headline3[:1]\n",
    "df3['Description']=description3[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43830f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1209f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "date4=[]\n",
    "author4=[]\n",
    "vertical4=[]\n",
    "headline4=[]\n",
    "description4=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    try:\n",
    "        date4.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        date4.append('-')\n",
    "        \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "    try:\n",
    "        author4.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        author4.append('-')\n",
    "    \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    try:\n",
    "        vertical4.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        vertical4.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "    try:\n",
    "        headline4.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        headline4.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    try:\n",
    "        description4.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        description4.append('-')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1453106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame({})\n",
    "df4['Date']=date4[:1]\n",
    "df4['Author']=author4[:1]\n",
    "df4['Vertical']=vertical4[:1]\n",
    "df4['Healines']=headline4[:1]\n",
    "df4['Description']=description4[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be640b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a97878c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date5=[]\n",
    "author5=[]\n",
    "vertical5=[]\n",
    "headline5=[]\n",
    "description5=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    try:\n",
    "        date5.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        date5.append('-')\n",
    "        \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "    try:\n",
    "        author5.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        author5.append('-')\n",
    "    \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    try:\n",
    "        vertical5.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        vertical5.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "    try:\n",
    "        headline5.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        headline5.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    try:\n",
    "        description5.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        description5.append('-')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69c7c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=pd.DataFrame({})\n",
    "df5['Date']=date5[:1]\n",
    "df5['Author']=author5[:1]\n",
    "df5['Vertical']=vertical5[:1]\n",
    "df5['Healines']=headline5[:1]\n",
    "df5['Description']=description5[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9a174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46bccaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "date6=[]\n",
    "author6=[]\n",
    "vertical6=[]\n",
    "headline6=[]\n",
    "description6=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    try:\n",
    "        date6.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        date6.append('-')\n",
    "        \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "    try:\n",
    "        author6.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        author6.append('-')\n",
    "    \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    try:\n",
    "        vertical6.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        vertical6.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "    try:\n",
    "        headline6.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        headline6.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    try:\n",
    "        description6.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        description6.append('-')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0499c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=pd.DataFrame({})\n",
    "df6['Date']=date6[:1]\n",
    "df6['Author']=author6[:1]\n",
    "df6['Vertical']=vertical6[:1]\n",
    "df6['Healines']=headline6[:1]\n",
    "df6['Description']=description6[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20036be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7211aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date7=[]\n",
    "author7=[]\n",
    "vertical7=[]\n",
    "headline7=[]\n",
    "description7=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    try:\n",
    "        date7.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        date7.append('-')\n",
    "        \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "    try:\n",
    "        author7.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        author7.append('-')\n",
    "    \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    try:\n",
    "        vertical7.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        vertical7.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "    try:\n",
    "        headline7.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        headline7.append('-')\n",
    "   \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    try:\n",
    "        description7.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        description7.append('-')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e1b8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7=pd.DataFrame({})\n",
    "df7['Date']=date7[:1]\n",
    "df7['Author']=author7[:1]\n",
    "df7['Vertical']=vertical7[:1]\n",
    "df7['Healines']=headline7[:1]\n",
    "df7['Description']=description7[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44052e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adb1621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date8=[]\n",
    "author8=[]\n",
    "vertical8=[]\n",
    "headline8=[]\n",
    "description8=[]\n",
    "\n",
    "# scrapping details 8 may 2021\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date8.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author8.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical8.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline8.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description8.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d778602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8=pd.DataFrame({})\n",
    "df8['Date']=date8[:1]\n",
    "df8['Author']=author8[:1]\n",
    "df8['Vertical']=vertical8[:1]\n",
    "df8['Healines']=headline8[:1]\n",
    "df8['Description']=description8[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ed0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb38523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date9=[]\n",
    "author9=[]\n",
    "vertical9=[]\n",
    "headline9=[]\n",
    "description9=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date9.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author9.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical9.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline9.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description9.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "795ae626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9=pd.DataFrame({})\n",
    "df9['Date']=date9[:1]\n",
    "df9['Author']=author9[:1]\n",
    "df9['Vertical']=vertical9[:1]\n",
    "df9['Healines']=headline9[:1]\n",
    "df9['Description']=description9[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b5784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3907887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date10=[]\n",
    "author10=[]\n",
    "vertical10=[]\n",
    "headline10=[]\n",
    "description10=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date10.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author10.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical10.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline10.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description10.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e61b689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10=pd.DataFrame({})\n",
    "df10['Date']=date10[:1]\n",
    "df10['Author']=author10[:1]\n",
    "df10['Vertical']=vertical10[:1]\n",
    "df10['Healines']=headline10[:1]\n",
    "df10['Description']=description10[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb1de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c201e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "date11=[]\n",
    "author11=[]\n",
    "vertical11=[]\n",
    "headline11=[]\n",
    "description11=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date11.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author11.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical11.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline11.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description11.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50f8ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11=pd.DataFrame({})\n",
    "df11['Date']=date11[:1]\n",
    "df11['Author']=author11[:1]\n",
    "df11['Vertical']=vertical11[:1]\n",
    "df11['Healines']=headline11[:1]\n",
    "df11['Description']=description11[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4863ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22aaade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "date12=[]\n",
    "author12=[]\n",
    "vertical12=[]\n",
    "headline12=[]\n",
    "description12=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date12.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author12.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical12.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline12.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description12.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "571c0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12=pd.DataFrame({})\n",
    "df12['Date']=date12[:1]\n",
    "df12['Author']=author12[:1]\n",
    "df12['Vertical']=vertical12[:1]\n",
    "df12['Healines']=headline12[:1]\n",
    "df12['Description']=description12[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1737b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d6c4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "date13=[]\n",
    "author13=[]\n",
    "vertical13=[]\n",
    "headline13=[]\n",
    "description13=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date13.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author13.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical13.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline13.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description13.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9ff7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df13=pd.DataFrame({})\n",
    "df13['Date']=date13[:1]\n",
    "df13['Author']=author13[:1]\n",
    "df13['Vertical']=vertical13[:1]\n",
    "df13['Healines']=headline13[:1]\n",
    "df13['Description']=description13[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4972530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92959c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date14=[]\n",
    "author14=[]\n",
    "vertical14=[]\n",
    "headline14=[]\n",
    "description14=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date14.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author14.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical14.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline14.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description14.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7bd9aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df14=pd.DataFrame({})\n",
    "df14['Date']=date14[:1]\n",
    "df14['Author']=author14[:1]\n",
    "df14['Vertical']=vertical14[:1]\n",
    "df14['Healines']=headline14[:1]\n",
    "df14['Description']=description14[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d37b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f95f7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "date15=[]\n",
    "author15=[]\n",
    "vertical15=[]\n",
    "headline15=[]\n",
    "description15=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date15.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author15.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical15.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline15.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description15.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38ecdd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df15=pd.DataFrame({})\n",
    "df15['Date']=date15[:1]\n",
    "df15['Author']=author15[:1]\n",
    "df15['Vertical']=vertical15[:1]\n",
    "df15['Healines']=headline15[:1]\n",
    "df15['Description']=description15[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb79d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2301c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "date16=[]\n",
    "author16=[]\n",
    "vertical16=[]\n",
    "headline16=[]\n",
    "description16=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date16.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author16.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical16.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline16.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description16.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bdac2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df16=pd.DataFrame({})\n",
    "df16['Date']=date16[:1]\n",
    "df16['Author']=author16[:1]\n",
    "df16['Vertical']=vertical16[:1]\n",
    "df16['Healines']=headline16[:1]\n",
    "df16['Description']=description16[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f545cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9f618ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "date17=[]\n",
    "author17=[]\n",
    "vertical17=[]\n",
    "headline17=[]\n",
    "description17=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date17.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author17.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical17.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline17.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description17.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39acd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df17=pd.DataFrame({})\n",
    "df17['Date']=date17[:1]\n",
    "df17['Author']=author17[:1]\n",
    "df17['Vertical']=vertical17[:1]\n",
    "df17['Healines']=headline17[:1]\n",
    "df17['Description']=description17[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7d5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9daeed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "date18=[]\n",
    "author18=[]\n",
    "vertical18=[]\n",
    "headline18=[]\n",
    "description18=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date18.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author18.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical18.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline18.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description18.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "068a85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df18=pd.DataFrame({})\n",
    "df18['Date']=date18[:1]\n",
    "df18['Author']=author18[:1]\n",
    "df18['Vertical']=vertical18[:1]\n",
    "df18['Healines']=headline18[:1]\n",
    "df18['Description']=description18[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23aceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54a762b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date19=[]\n",
    "author19=[]\n",
    "vertical19=[]\n",
    "headline19=[]\n",
    "description19=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date19.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author19.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical19.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline19.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description19.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3d043a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df19=pd.DataFrame({})\n",
    "df19['Date']=date19[:1]\n",
    "df19['Author']=author19[:1]\n",
    "df19['Vertical']=vertical19[:1]\n",
    "df19['Healines']=headline19[:1]\n",
    "df19['Description']=description19[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0083609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4e89d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date20=[]\n",
    "author20=[]\n",
    "vertical20=[]\n",
    "headline20=[]\n",
    "description20=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date20.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author20.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical20.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline20.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description20.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a3f2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df20=pd.DataFrame({})\n",
    "df20['Date']=date20[:1]\n",
    "df20['Author']=author20[:1]\n",
    "df20['Vertical']=vertical20[:1]\n",
    "df20['Healines']=headline20[:1]\n",
    "df20['Description']=description20[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cb95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c2ba823",
   "metadata": {},
   "outputs": [],
   "source": [
    "date21=[]\n",
    "author21=[]\n",
    "vertical21=[]\n",
    "headline21=[]\n",
    "description21=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date21.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author21.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical21.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline21.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description21.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "409ac6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df21=pd.DataFrame({})\n",
    "df21['Date']=date21[:1]\n",
    "df21['Author']=author21[:1]\n",
    "df21['Vertical']=vertical21[:1]\n",
    "df21['Healines']=headline21[:1]\n",
    "df21['Description']=description21[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffccf7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0961b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "date22=[]\n",
    "author22=[]\n",
    "vertical22=[]\n",
    "headline22=[]\n",
    "description22=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date22.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author22.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical22.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline22.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description22.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8381772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df22=pd.DataFrame({})\n",
    "df22['Date']=date22[:1]\n",
    "df22['Author']=author22[:1]\n",
    "df22['Vertical']=vertical22[:1]\n",
    "df22['Healines']=headline22[:1]\n",
    "df22['Description']=description22[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec1800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3d5c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "date23=[]\n",
    "author23=[]\n",
    "vertical23=[]\n",
    "headline23=[]\n",
    "description23=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date23.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author23.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical23.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline23.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description23.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "daaa7f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df23=pd.DataFrame({})\n",
    "df23['Date']=date23[:1]\n",
    "df23['Author']=author23[:1]\n",
    "df23['Vertical']=vertical23[:1]\n",
    "df23['Healines']=headline23[:1]\n",
    "df23['Description']=description23[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42c2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf21dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date24=[]\n",
    "author24=[]\n",
    "vertical24=[]\n",
    "headline24=[]\n",
    "description24=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date24.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author24.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical24.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline24.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description24.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5aaba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df24=pd.DataFrame({})\n",
    "df24['Date']=date24[:1]\n",
    "df24['Author']=author24[:1]\n",
    "df24['Vertical']=vertical24[:1]\n",
    "df24['Healines']=headline24[:1]\n",
    "df24['Description']=description24[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8894a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "817b6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "date25=[]\n",
    "author25=[]\n",
    "vertical25=[]\n",
    "headline25=[]\n",
    "description25=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date25.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author25.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical25.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline25.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description25.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e0c47647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df25=pd.DataFrame({})\n",
    "df25['Date']=date25[:1]\n",
    "df25['Author']=author25[:1]\n",
    "df25['Vertical']=vertical25[:1]\n",
    "df25['Healines']=headline25[:1]\n",
    "df25['Description']=description25[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f95f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6dee06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "date26=[]\n",
    "author26=[]\n",
    "vertical26=[]\n",
    "headline26=[]\n",
    "description26=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date26.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author26.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical26.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline26.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description26.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a9eddbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df26=pd.DataFrame({})\n",
    "df26['Date']=date26[:1]\n",
    "df26['Author']=author26[:1]\n",
    "df26['Vertical']=vertical26[:1]\n",
    "df26['Healines']=headline26[:1]\n",
    "df26['Description']=description26[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99c586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc3b17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "date27=[]\n",
    "author27=[]\n",
    "vertical27=[]\n",
    "headline27=[]\n",
    "description27=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date27.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author27.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical27.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline27.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description27.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d3649d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df27=pd.DataFrame({})\n",
    "df27['Date']=date27[:1]\n",
    "df27['Author']=author27[:1]\n",
    "df27['Vertical']=vertical27[:1]\n",
    "df27['Healines']=headline27[:1]\n",
    "df27['Description']=description27[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539c4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b29921c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "date28=[]\n",
    "author28=[]\n",
    "vertical28=[]\n",
    "headline28=[]\n",
    "description28=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date28.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author28.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical28.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline28.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description28.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9777e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df28=pd.DataFrame({})\n",
    "df28['Date']=date28[:1]\n",
    "df28['Author']=author28[:1]\n",
    "df28['Vertical']=vertical28[:1]\n",
    "df28['Healines']=headline28[:1]\n",
    "df28['Description']=description28[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd41b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f32d64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "date29=[]\n",
    "author29=[]\n",
    "vertical29=[]\n",
    "headline29=[]\n",
    "description29=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date29.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author29.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical29.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline29.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description29.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c2873641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df29=pd.DataFrame({})\n",
    "df29['Date']=date29[:1]\n",
    "df29['Author']=author29[:1]\n",
    "df29['Vertical']=vertical29[:1]\n",
    "df29['Healines']=headline29[:1]\n",
    "df29['Description']=description29[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca574f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1711f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "date30=[]\n",
    "author30=[]\n",
    "vertical30=[]\n",
    "headline30=[]\n",
    "description30=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date30.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author30.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical30.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline30.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description30.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6cd2afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df30=pd.DataFrame({})\n",
    "df30['Date']=date30[:1]\n",
    "df30['Author']=author30[:1]\n",
    "df30['Vertical']=vertical30[:1]\n",
    "df30['Healines']=headline30[:1]\n",
    "df30['Description']=description30[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292eac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0853b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "date31=[]\n",
    "author31=[]\n",
    "vertical31=[]\n",
    "headline31=[]\n",
    "description31=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date31.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author31.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical31.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline31.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description31.append(i.text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6d93218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df31=pd.DataFrame({})\n",
    "df31['Date']=date31[:1]\n",
    "df31['Author']=author31[:1]\n",
    "df31['Vertical']=vertical31[:1]\n",
    "df31['Healines']=headline31[:1]\n",
    "df31['Description']=description31[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953f1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a21b5a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "date32=[]\n",
    "author32=[]\n",
    "vertical32=[]\n",
    "headline32=[]\n",
    "description32=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date32.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author32.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical32.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline32.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description32.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "568d144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df32=pd.DataFrame({})\n",
    "df32['Date']=date32[:1]\n",
    "df32['Author']=author32[:1]\n",
    "df32['Vertical']=vertical32[:1]\n",
    "df32['Healines']=headline32[:1]\n",
    "df32['Description']=description32[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb426b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ee437324",
   "metadata": {},
   "outputs": [],
   "source": [
    "date33=[]\n",
    "author33=[]\n",
    "vertical33=[]\n",
    "headline33=[]\n",
    "description33=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date33.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author33.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical33.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline33.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description33.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d2874270",
   "metadata": {},
   "outputs": [],
   "source": [
    "df33=pd.DataFrame({})\n",
    "df33['Date']=date33[:1]\n",
    "df33['Author']=author33[:1]\n",
    "df33['Vertical']=vertical33[:1]\n",
    "df33['Healines']=headline33[:1]\n",
    "df33['Description']=description33[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0dc412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6834a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date34=[]\n",
    "author34=[]\n",
    "vertical34=[]\n",
    "headline34=[]\n",
    "description34=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date34.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author34.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical34.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline34.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description34.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ec7cc185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df34=pd.DataFrame({})\n",
    "df34['Date']=date34[:1]\n",
    "df34['Author']=author34[:1]\n",
    "df34['Vertical']=vertical34[:1]\n",
    "df34['Healines']=headline34[:1]\n",
    "df34['Description']=description34[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cac1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c1ec7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date35=[]\n",
    "author35=[]\n",
    "vertical35=[]\n",
    "headline35=[]\n",
    "description35=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date35.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author35.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical35.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline35.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description35.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "38066759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df35=pd.DataFrame({})\n",
    "df35['Date']=date35[:1]\n",
    "df35['Author']=author35[:1]\n",
    "df35['Vertical']=vertical35[:1]\n",
    "df35['Healines']=headline35[:1]\n",
    "df35['Description']=description35[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a2103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a716aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "date36=[]\n",
    "author36=[]\n",
    "vertical36=[]\n",
    "headline36=[]\n",
    "description36=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date36.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author36.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical36.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline36.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description36.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "98021eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df36=pd.DataFrame({})\n",
    "df36['Date']=date36[:1]\n",
    "df36['Author']=author36[:1]\n",
    "df36['Vertical']=vertical36[:1]\n",
    "df36['Healines']=headline36[:1]\n",
    "df36['Description']=description36[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f5222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4bc78bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "date37=[]\n",
    "author37=[]\n",
    "vertical37=[]\n",
    "headline37=[]\n",
    "description37=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date37.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author37.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical37.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline37.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description37.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5097f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df37=pd.DataFrame({})\n",
    "df37['Date']=date37[:1]\n",
    "df37['Author']=author37[:1]\n",
    "df37['Vertical']=vertical37[:1]\n",
    "df37['Healines']=headline37[:1]\n",
    "df37['Description']=description37[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64722ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "89017b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "date38=[]\n",
    "author38=[]\n",
    "vertical38=[]\n",
    "headline38=[]\n",
    "description38=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date38.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author38.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical38.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline38.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description38.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cffd6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df38=pd.DataFrame({})\n",
    "df38['Date']=date38[:1]\n",
    "df38['Author']=author38[:1]\n",
    "df38['Vertical']=vertical38[:1]\n",
    "df38['Healines']=headline38[:1]\n",
    "df38['Description']=description38[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45676db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cb6aae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date39=[]\n",
    "author39=[]\n",
    "vertical39=[]\n",
    "headline39=[]\n",
    "description39=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date39.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author39.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical39.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline39.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description39.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "22a44873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df39=pd.DataFrame({})\n",
    "df39['Date']=date39[:1]\n",
    "df39['Author']=author39[:1]\n",
    "df39['Vertical']=vertical39[:1]\n",
    "df39['Healines']=headline39[:1]\n",
    "df39['Description']=description39[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ec487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d8fe2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date40=[]\n",
    "author40=[]\n",
    "vertical40=[]\n",
    "headline40=[]\n",
    "description40=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date40.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author40.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical40.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline40.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description40.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "02cb68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df40=pd.DataFrame({})\n",
    "df40['Date']=date40[:1]\n",
    "df40['Author']=author40[:1]\n",
    "df40['Vertical']=vertical40[:1]\n",
    "df40['Healines']=headline40[:1]\n",
    "df40['Description']=description40[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c3eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "41857513",
   "metadata": {},
   "outputs": [],
   "source": [
    "date41=[]\n",
    "author41=[]\n",
    "vertical41=[]\n",
    "headline41=[]\n",
    "description41=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date41.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author41.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical41.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline41.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description41.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1ac46c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df41=pd.DataFrame({})\n",
    "df41['Date']=date41[:1]\n",
    "df41['Author']=author41[:1]\n",
    "df41['Vertical']=vertical41[:1]\n",
    "df41['Healines']=headline41[:1]\n",
    "df41['Description']=description41[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0f85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bcc86357",
   "metadata": {},
   "outputs": [],
   "source": [
    "date42=[]\n",
    "author42=[]\n",
    "vertical42=[]\n",
    "headline42=[]\n",
    "description42=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date42.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author42.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical42.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline42.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description42.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fe9858e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df42=pd.DataFrame({})\n",
    "df42['Date']=date42[:1]\n",
    "df42['Author']=author42[:1]\n",
    "df42['Vertical']=vertical42[:1]\n",
    "df42['Healines']=headline42[:1]\n",
    "df42['Description']=description42[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cebee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "296118d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date43=[]\n",
    "author43=[]\n",
    "vertical43=[]\n",
    "headline43=[]\n",
    "description43=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date43.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author43.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical43.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline43.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description43.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2fa92775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df43=pd.DataFrame({})\n",
    "df43['Date']=date43[:1]\n",
    "df43['Author']=author43[:1]\n",
    "df43['Vertical']=vertical43[:1]\n",
    "df43['Healines']=headline43[:1]\n",
    "df43['Description']=description43[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a751c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "09f1b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "date44=[]\n",
    "author44=[]\n",
    "vertical44=[]\n",
    "headline44=[]\n",
    "description44=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date44.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author44.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical44.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline44.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description44.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "792ce463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df44=pd.DataFrame({})\n",
    "df44['Date']=date44[:1]\n",
    "df44['Author']=author44[:1]\n",
    "df44['Vertical']=vertical44[:1]\n",
    "df44['Healines']=headline44[:1]\n",
    "df44['Description']=description44[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad083dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0953961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date45=[]\n",
    "author45=[]\n",
    "vertical45=[]\n",
    "headline45=[]\n",
    "description45=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date45.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author45.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical45.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline45.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description45.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "40a9f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df45=pd.DataFrame({})\n",
    "df45['Date']=date45[:1]\n",
    "df45['Author']=author45[:1]\n",
    "df45['Vertical']=vertical45[:1]\n",
    "df45['Healines']=headline45[:1]\n",
    "df45['Description']=description45[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116117a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "84810319",
   "metadata": {},
   "outputs": [],
   "source": [
    "date46=[]\n",
    "author46=[]\n",
    "vertical46=[]\n",
    "headline46=[]\n",
    "description46=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date46.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author46.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical46.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline46.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description46.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "566741c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df46=pd.DataFrame({})\n",
    "df46['Date']=date46[:1]\n",
    "df46['Author']=author46[:1]\n",
    "df46['Vertical']=vertical46[:1]\n",
    "df46['Healines']=headline46[:1]\n",
    "df46['Description']=description46[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ea79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2f46bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date47=[]\n",
    "author47=[]\n",
    "vertical47=[]\n",
    "headline47=[]\n",
    "description47=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date47.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author47.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical47.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline47.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description47.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "15b33bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df47=pd.DataFrame({})\n",
    "df47['Date']=date47[:1]\n",
    "df47['Author']=author47[:1]\n",
    "df47['Vertical']=vertical47[:1]\n",
    "df47['Healines']=headline47[:1]\n",
    "df47['Description']=description47[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6e646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a1c7a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "date48=[]\n",
    "author48=[]\n",
    "vertical48=[]\n",
    "headline48=[]\n",
    "description48=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date48.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author48.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical48.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline48.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description48.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7f5b709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df48=pd.DataFrame({})\n",
    "df48['Date']=date48[:1]\n",
    "df48['Author']=author48[:1]\n",
    "df48['Vertical']=vertical48[:1]\n",
    "df48['Healines']=headline48[:1]\n",
    "df48['Description']=description48[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153a273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "35cea214",
   "metadata": {},
   "outputs": [],
   "source": [
    "date49=[]\n",
    "author49=[]\n",
    "vertical49=[]\n",
    "headline49=[]\n",
    "description49=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date49.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author49.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical49.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline49.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description49.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2faddf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df49=pd.DataFrame({})\n",
    "df49['Date']=date49[:1]\n",
    "df49['Author']=author49[:1]\n",
    "df49['Vertical']=vertical49[:1]\n",
    "df49['Healines']=headline49[:1]\n",
    "df49['Description']=description49[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576e473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1272f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date50=[]\n",
    "author50=[]\n",
    "vertical50=[]\n",
    "headline50=[]\n",
    "description50=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date50.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author50.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical50.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline50.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description50.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "85364b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df50=pd.DataFrame({})\n",
    "df50['Date']=date50[:1]\n",
    "df50['Author']=author50[:1]\n",
    "df50['Vertical']=vertical50[:1]\n",
    "df50['Healines']=headline50[:1]\n",
    "df50['Description']=description50[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d56d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "351be78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date51=[]\n",
    "author51=[]\n",
    "vertical51=[]\n",
    "headline51=[]\n",
    "description51=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date51.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author51.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical51.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline51.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description51.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "24dd9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df51=pd.DataFrame({})\n",
    "df51['Date']=date51[:1]\n",
    "df51['Author']=author51[:1]\n",
    "df51['Vertical']=vertical51[:1]\n",
    "df51['Healines']=headline51[:1]\n",
    "df51['Description']=description51[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5381947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date52=[]\n",
    "author52=[]\n",
    "vertical52=[]\n",
    "headline52=[]\n",
    "description52=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date52.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author52.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical52.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline52.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description52.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "218a93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df52=pd.DataFrame({})\n",
    "df52['Date']=date52[:1]\n",
    "df52['Author']=author52[:1]\n",
    "df52['Vertical']=vertical52[:1]\n",
    "df52['Healines']=headline52[:1]\n",
    "df52['Description']=description52[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943acce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c8fb08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date53=[]\n",
    "author53=[]\n",
    "vertical53=[]\n",
    "headline53=[]\n",
    "description53=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date53.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author53.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical53.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline53.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description53.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1b2e8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df53=pd.DataFrame({})\n",
    "df53['Date']=date53[:1]\n",
    "df53['Author']=author53[:1]\n",
    "df53['Vertical']=vertical53[:1]\n",
    "df53['Healines']=headline53[:1]\n",
    "df53['Description']=description5[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68191b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "531565dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "date54=[]\n",
    "author54=[]\n",
    "vertical54=[]\n",
    "headline54=[]\n",
    "description54=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date54.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author54.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical54.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline54.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description54.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "97021f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df54=pd.DataFrame({})\n",
    "df54['Date']=date54[:1]\n",
    "df54['Author']=author54[:1]\n",
    "df54['Vertical']=vertical54[:1]\n",
    "df54['Healines']=headline54[:1]\n",
    "df54['Description']=description54[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a45842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5b290b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date55=[]\n",
    "author55=[]\n",
    "vertical55=[]\n",
    "headline55=[]\n",
    "description55=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date55.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author55.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical55.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline55.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description55.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "da1641c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df55=pd.DataFrame({})\n",
    "df55['Date']=date55[:1]\n",
    "df55['Author']=author55[:1]\n",
    "df55['Vertical']=vertical55[:1]\n",
    "df55['Healines']=headline55[:1]\n",
    "df55['Description']=description55[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4ffb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "eda0e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "date56=[]\n",
    "author56=[]\n",
    "vertical56=[]\n",
    "headline56=[]\n",
    "description56=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date56.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author56.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical56.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline56.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description56.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9192db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df56=pd.DataFrame({})\n",
    "df56['Date']=date56[:1]\n",
    "df56['Author']=author56[:1]\n",
    "df56['Vertical']=vertical56[:1]\n",
    "df56['Healines']=headline56[:1]\n",
    "df56['Description']=description56[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa8004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ee8deeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "date57=[]\n",
    "author57=[]\n",
    "vertical57=[]\n",
    "headline57=[]\n",
    "description57=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date57.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author57.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical57.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline57.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description57.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c81290d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df57=pd.DataFrame({})\n",
    "df57['Date']=date57[:1]\n",
    "df57['Author']=author57[:1]\n",
    "df57['Vertical']=vertical57[:1]\n",
    "df57['Healines']=headline57[:1]\n",
    "df57['Description']=description57[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739cdc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "649ac169",
   "metadata": {},
   "outputs": [],
   "source": [
    "date58=[]\n",
    "author58=[]\n",
    "vertical58=[]\n",
    "headline58=[]\n",
    "description58=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date58.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author58.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical58.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline58.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description58.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "16cc4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df58=pd.DataFrame({})\n",
    "df58['Date']=date58[:1]\n",
    "df58['Author']=author58[:1]\n",
    "df58['Vertical']=vertical58[:1]\n",
    "df58['Healines']=headline58[:1]\n",
    "df58['Description']=description58[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0811c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b81f4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date59=[]\n",
    "author59=[]\n",
    "vertical59=[]\n",
    "headline59=[]\n",
    "description59=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date59.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author59.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical59.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline59.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description59.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d6f82af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df59=pd.DataFrame({})\n",
    "df59['Date']=date59[:1]\n",
    "df59['Author']=author59[:1]\n",
    "df59['Vertical']=vertical59[:1]\n",
    "df59['Healines']=headline59[:1]\n",
    "df59['Description']=description59[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58aeb12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "968cea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "date60=[]\n",
    "author60=[]\n",
    "vertical60=[]\n",
    "headline60=[]\n",
    "description60=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date60.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author60.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical60.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline60.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description60.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2619d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df60=pd.DataFrame({})\n",
    "df60['Date']=date60[:1]\n",
    "df60['Author']=author60[:1]\n",
    "df60['Vertical']=vertical60[:1]\n",
    "df60['Healines']=headline60[:1]\n",
    "df60['Description']=description60[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb6920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "352488fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "date61=[]\n",
    "author61=[]\n",
    "vertical61=[]\n",
    "headline61=[]\n",
    "description61=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date61.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author61.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical61.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline61.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description61.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "258e4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df61=pd.DataFrame({})\n",
    "df61['Date']=date61[:1]\n",
    "df61['Author']=author61[:1]\n",
    "df61['Vertical']=vertical61[:1]\n",
    "df61['Healines']=headline61[:1]\n",
    "df61['Description']=description61[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9473ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a6e8f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date62=[]\n",
    "author62=[]\n",
    "vertical62=[]\n",
    "headline62=[]\n",
    "description62=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date62.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author62.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical62.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline62.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description62.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b2d79215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df62=pd.DataFrame({})\n",
    "df62['Date']=date62[:1]\n",
    "df62['Author']=author62[:1]\n",
    "df62['Vertical']=vertical62[:1]\n",
    "df62['Healines']=headline62[:1]\n",
    "df62['Description']=description62[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f163c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "22d56d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "date63=[]\n",
    "author63=[]\n",
    "vertical63=[]\n",
    "headline63=[]\n",
    "description63=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date63.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author63.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical63.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline63.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description63.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9a0066c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df63=pd.DataFrame({})\n",
    "df63['Date']=date63[:1]\n",
    "df63['Author']=author63[:1]\n",
    "df63['Vertical']=vertical63[:1]\n",
    "df63['Healines']=headline63[:1]\n",
    "df63['Description']=description63[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149233b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8b91ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date64=[]\n",
    "author64=[]\n",
    "vertical64=[]\n",
    "headline64=[]\n",
    "description64=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date64.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author64.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical64.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline64.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description64.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "36571839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df64=pd.DataFrame({})\n",
    "df64['Date']=date64[:1]\n",
    "df64['Author']=author64[:1]\n",
    "df64['Vertical']=vertical64[:1]\n",
    "df64['Healines']=headline64[:1]\n",
    "df64['Description']=description64[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c30cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5f3aabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "date65=[]\n",
    "author65=[]\n",
    "vertical65=[]\n",
    "headline65=[]\n",
    "description65=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date65.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author65.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical65.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline65.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description65.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0f4f86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df65=pd.DataFrame({})\n",
    "df65['Date']=date65[:1]\n",
    "df65['Author']=author65[:1]\n",
    "df65['Vertical']=vertical65[:1]\n",
    "df65['Healines']=headline65[:1]\n",
    "df65['Description']=description65[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a56b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e2444add",
   "metadata": {},
   "outputs": [],
   "source": [
    "date66=[]\n",
    "author66=[]\n",
    "vertical66=[]\n",
    "headline66=[]\n",
    "description66=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date66.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author66.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical66.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline66.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description66.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c08396ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df66=pd.DataFrame({})\n",
    "df66['Date']=date66[:1]\n",
    "df66['Author']=author66[:1]\n",
    "df66['Vertical']=vertical66[:1]\n",
    "df66['Healines']=headline66[:1]\n",
    "df66['Description']=description66[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc8ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ac55acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date67=[]\n",
    "author67=[]\n",
    "vertical67=[]\n",
    "headline67=[]\n",
    "description67=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date67.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author67.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical67.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline67.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description67.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "832d491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df67=pd.DataFrame({})\n",
    "df67['Date']=date67[:1]\n",
    "df67['Author']=author67[:1]\n",
    "df67['Vertical']=vertical67[:1]\n",
    "df67['Healines']=headline67[:1]\n",
    "df67['Description']=description67[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd3b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ed54330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date68=[]\n",
    "author68=[]\n",
    "vertical68=[]\n",
    "headline68=[]\n",
    "description68=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date68.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author68.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical68.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline68.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description68.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6edcdd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df68=pd.DataFrame({})\n",
    "df68['Date']=date68[:1]\n",
    "df68['Author']=author68[:1]\n",
    "df68['Vertical']=vertical68[:1]\n",
    "df68['Healines']=headline68[:1]\n",
    "df68['Description']=description68[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e282e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "edfa0500",
   "metadata": {},
   "outputs": [],
   "source": [
    "date69=[]\n",
    "author69=[]\n",
    "vertical69=[]\n",
    "headline69=[]\n",
    "description69=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date69.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author69.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical69.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline69.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description69.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "49872115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df69=pd.DataFrame({})\n",
    "df69['Date']=date69[:1]\n",
    "df69['Author']=author69[:1]\n",
    "df69['Vertical']=vertical69[:1]\n",
    "df69['Healines']=headline69[:1]\n",
    "df69['Description']=description69[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c0798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5ca4e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "date70=[]\n",
    "author70=[]\n",
    "vertical70=[]\n",
    "headline70=[]\n",
    "description70=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date70.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author70.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical70.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline70.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description70.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "75866139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df70=pd.DataFrame({})\n",
    "df70['Date']=date70[:1]\n",
    "df70['Author']=author70[:1]\n",
    "df70['Vertical']=vertical70[:1]\n",
    "df70['Healines']=headline70[:1]\n",
    "df70['Description']=description70[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c45bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bcb7ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "date71=[]\n",
    "author71=[]\n",
    "vertical71=[]\n",
    "headline71=[]\n",
    "description71=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date71.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author71.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical71.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline71.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description71.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a73a6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df71=pd.DataFrame({})\n",
    "df71['Date']=date71[:1]\n",
    "df71['Author']=author71[:1]\n",
    "df71['Vertical']=vertical71[:1]\n",
    "df71['Healines']=headline71[:1]\n",
    "df71['Description']=description71[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146fd69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3ff87512",
   "metadata": {},
   "outputs": [],
   "source": [
    "date72=[]\n",
    "author72=[]\n",
    "vertical72=[]\n",
    "headline72=[]\n",
    "description72=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date72.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author72.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical72.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline72.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description72.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b2e9bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df72=pd.DataFrame({})\n",
    "df72['Date']=date72[:1]\n",
    "df72['Author']=author72[:1]\n",
    "df72['Vertical']=vertical72[:1]\n",
    "df72['Healines']=headline72[:1]\n",
    "df72['Description']=description72[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc9766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6680f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date73=[]\n",
    "author73=[]\n",
    "vertical73=[]\n",
    "headline73=[]\n",
    "description73=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date73.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author73.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical73.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline73.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description73.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "99d87a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df73=pd.DataFrame({})\n",
    "df73['Date']=date73[:1]\n",
    "df73['Author']=author73[:1]\n",
    "df73['Vertical']=vertical73[:1]\n",
    "df73['Healines']=headline73[:1]\n",
    "df73['Description']=description73[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f00895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cf7a3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "date74=[]\n",
    "author74=[]\n",
    "vertical74=[]\n",
    "headline74=[]\n",
    "description74=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date74.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author74.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical74.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline74.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description74.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9a137cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df74=pd.DataFrame({})\n",
    "df74['Date']=date74[:1]\n",
    "df74['Author']=author74[:1]\n",
    "df74['Vertical']=vertical74[:1]\n",
    "df74['Healines']=headline74[:1]\n",
    "df74['Description']=description74[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f86e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "64d24c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "date75=[]\n",
    "author75=[]\n",
    "vertical75=[]\n",
    "headline75=[]\n",
    "description75=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date75.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author75.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical75.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline75.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description75.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "315f12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df75=pd.DataFrame({})\n",
    "df75['Date']=date75[:1]\n",
    "df75['Author']=author75[:1]\n",
    "df75['Vertical']=vertical75[:1]\n",
    "df75['Healines']=headline75[:1]\n",
    "df75['Description']=description75[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8955df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "23274a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "date76=[]\n",
    "author76=[]\n",
    "vertical76=[]\n",
    "headline76=[]\n",
    "description76=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date76.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author76.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical76.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline76.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description76.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "71916968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df76=pd.DataFrame({})\n",
    "df76['Date']=date76[:1]\n",
    "df76['Author']=author76[:1]\n",
    "df76['Vertical']=vertical76[:1]\n",
    "df76['Healines']=headline76[:1]\n",
    "df76['Description']=description76[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7667d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2cfcad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "date77=[]\n",
    "author77=[]\n",
    "vertical77=[]\n",
    "headline77=[]\n",
    "description77=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date77.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author77.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical77.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline77.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description77.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "65da4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df77=pd.DataFrame({})\n",
    "df77['Date']=date77[:1]\n",
    "df77['Author']=author77[:1]\n",
    "df77['Vertical']=vertical77[:1]\n",
    "df77['Healines']=headline77[:1]\n",
    "df77['Description']=description77[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fe18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "200251c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date78=[]\n",
    "author78=[]\n",
    "vertical78=[]\n",
    "headline78=[]\n",
    "description78=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date78.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author78.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical78.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline78.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description78.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ff3fa72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df78=pd.DataFrame({})\n",
    "df78['Date']=date78[:1]\n",
    "df78['Author']=author78[:1]\n",
    "df78['Vertical']=vertical78[:1]\n",
    "df78['Healines']=headline78[:1]\n",
    "df78['Description']=description78[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac98b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "23a712ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "date79=[]\n",
    "author79=[]\n",
    "vertical79=[]\n",
    "headline79=[]\n",
    "description79=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date79.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author79.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical79.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline79.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description79.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0b9fad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df79=pd.DataFrame({})\n",
    "df79['Date']=date79[:1]\n",
    "df79['Author']=author79[:1]\n",
    "df79['Vertical']=vertical79[:1]\n",
    "df79['Healines']=headline79[:1]\n",
    "df79['Description']=description79[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71270e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c6fee3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "date80=[]\n",
    "author80=[]\n",
    "vertical80=[]\n",
    "headline80=[]\n",
    "description80=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date80.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author80.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical80.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline80.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description80.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "dabf6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df80=pd.DataFrame({})\n",
    "df80['Date']=date80[:1]\n",
    "df80['Author']=author80[:1]\n",
    "df80['Vertical']=vertical80[:1]\n",
    "df80['Healines']=headline80[:1]\n",
    "df80['Description']=description80[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f72aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cf6b89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "date81=[]\n",
    "author81=[]\n",
    "vertical81=[]\n",
    "headline81=[]\n",
    "description81=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date81.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author81.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical81.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline81.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description81.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b44e0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df81=pd.DataFrame({})\n",
    "df81['Date']=date81[:1]\n",
    "df81['Author']=author81[:1]\n",
    "df81['Vertical']=vertical81[:1]\n",
    "df81['Healines']=headline81[:1]\n",
    "df81['Description']=description81[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96af641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5297f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date82=[]\n",
    "author82=[]\n",
    "vertical82=[]\n",
    "headline82=[]\n",
    "description82=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date82.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author82.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical82.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline82.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description82.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "21a642e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df82=pd.DataFrame({})\n",
    "df82['Date']=date82[:1]\n",
    "df82['Author']=author82[:1]\n",
    "df82['Vertical']=vertical82[:1]\n",
    "df82['Healines']=headline82[:1]\n",
    "df82['Description']=description82[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbc273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a1180889",
   "metadata": {},
   "outputs": [],
   "source": [
    "date83=[]\n",
    "author83=[]\n",
    "vertical83=[]\n",
    "headline83=[]\n",
    "description83=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date83.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author83.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical83.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline83.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description83.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ae70bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df83=pd.DataFrame({})\n",
    "df83['Date']=date83[:1]\n",
    "df83['Author']=author83[:1]\n",
    "df83['Vertical']=vertical83[:1]\n",
    "df83['Healines']=headline83[:1]\n",
    "df83['Description']=description83[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11735a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fc47a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date84=[]\n",
    "author84=[]\n",
    "vertical84=[]\n",
    "headline84=[]\n",
    "description84=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date84.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author84.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical84.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline84.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description84.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "da7fa1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df84=pd.DataFrame({})\n",
    "df84['Date']=date84[:1]\n",
    "df84['Author']=author84[:1]\n",
    "df84['Vertical']=vertical84[:1]\n",
    "df84['Healines']=headline84[:1]\n",
    "df84['Description']=description84[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9957d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "84837023",
   "metadata": {},
   "outputs": [],
   "source": [
    "date85=[]\n",
    "author85=[]\n",
    "vertical85=[]\n",
    "headline85=[]\n",
    "description85=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date85.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author85.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//ul[@class='list-unstyled']\")\n",
    "for i in ve:\n",
    "    vertical85.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline85.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description85.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c362b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df85=pd.DataFrame({})\n",
    "df85['Date']=date85[:1]\n",
    "df85['Author']=author85[:1]\n",
    "df85['Vertical']=vertical85[:1]\n",
    "df85['Healines']=headline85[:1]\n",
    "df85['Description']=description85[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e850535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ee790d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "date86=[]\n",
    "author86=[]\n",
    "vertical86=[]\n",
    "headline86=[]\n",
    "description86=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date86.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author86.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical86.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline86.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description86.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2d13f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df86=pd.DataFrame({})\n",
    "df86['Date']=date86[:1]\n",
    "df86['Author']=author86[:1]\n",
    "df86['Vertical']=vertical86[:1]\n",
    "df86['Healines']=headline86[:1]\n",
    "df86['Description']=description86[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb135b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5a711e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "date87=[]\n",
    "author87=[]\n",
    "vertical87=[]\n",
    "headline87=[]\n",
    "description87=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date87.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author87.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical87.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline87.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description87.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5b8c8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df87=pd.DataFrame({})\n",
    "df87['Date']=date87[:1]\n",
    "df87['Author']=author87[:1]\n",
    "df87['Vertical']=vertical87[:1]\n",
    "df87['Healines']=headline87[:1]\n",
    "df87['Description']=description87[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66c705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c91f8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "date88=[]\n",
    "author88=[]\n",
    "vertical88=[]\n",
    "headline88=[]\n",
    "description88=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date88.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author88.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical88.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline88.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description88.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "950758d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df88=pd.DataFrame({})\n",
    "df88['Date']=date88[:1]\n",
    "df88['Author']=author88[:1]\n",
    "df88['Vertical']=vertical88[:1]\n",
    "df88['Healines']=headline88[:1]\n",
    "df88['Description']=description88[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab1e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5661f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date89=[]\n",
    "author89=[]\n",
    "vertical89=[]\n",
    "headline89=[]\n",
    "description89=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date89.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author89.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical89.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline89.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description89.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f6c92e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df89=pd.DataFrame({})\n",
    "df89['Date']=date89[:1]\n",
    "df89['Author']=author89[:1]\n",
    "df89['Vertical']=vertical89[:1]\n",
    "df89['Healines']=headline89[:1]\n",
    "df89['Description']=description89[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e91da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a3c20ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "date90=[]\n",
    "author90=[]\n",
    "vertical90=[]\n",
    "headline90=[]\n",
    "description90=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date90.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author90.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical90.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline90.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description90.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "331bae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df90=pd.DataFrame({})\n",
    "df90['Date']=date90[:1]\n",
    "df90['Author']=author90[:1]\n",
    "df90['Vertical']=vertical90[:1]\n",
    "df90['Healines']=headline90[:1]\n",
    "df90['Description']=description90[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03741a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dd242218",
   "metadata": {},
   "outputs": [],
   "source": [
    "date91=[]\n",
    "author91=[]\n",
    "vertical91=[]\n",
    "headline91=[]\n",
    "description91=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date91.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author91.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical91.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline91.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description91.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "92717cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df91=pd.DataFrame({})\n",
    "df91['Date']=date91[:1]\n",
    "df91['Author']=author91[:1]\n",
    "df91['Vertical']=vertical91[:1]\n",
    "df91['Healines']=headline91[:1]\n",
    "df91['Description']=description91[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac3b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "85d85c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "date92=[]\n",
    "author92=[]\n",
    "vertical92=[]\n",
    "headline92=[]\n",
    "description92=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date92.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author92.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical92.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline92.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description92.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "92968366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df92=pd.DataFrame({})\n",
    "df92['Date']=date92[:1]\n",
    "df92['Author']=author92[:1]\n",
    "df92['Vertical']=vertical92[:1]\n",
    "df92['Healines']=headline92[:1]\n",
    "df92['Description']=description92[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5de062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3b05ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "date93=[]\n",
    "author93=[]\n",
    "vertical93=[]\n",
    "headline93=[]\n",
    "description93=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date93.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author93.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical93.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline93.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description93.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "714da6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df93=pd.DataFrame({})\n",
    "df93['Date']=date93[:1]\n",
    "df93['Author']=author93[:1]\n",
    "df93['Vertical']=vertical93[:1]\n",
    "df93['Healines']=headline93[:1]\n",
    "df93['Description']=description93[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14ec37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fd045023",
   "metadata": {},
   "outputs": [],
   "source": [
    "date94=[]\n",
    "author94=[]\n",
    "vertical94=[]\n",
    "headline94=[]\n",
    "description94=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date94.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author94.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical94.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline94.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description94.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "760c2f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df94=pd.DataFrame({})\n",
    "df94['Date']=date94[:1]\n",
    "df94['Author']=author94[:1]\n",
    "df94['Vertical']=vertical94[:1]\n",
    "df94['Healines']=headline94[:1]\n",
    "df94['Description']=description94[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6e967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "56958aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "date95=[]\n",
    "author95=[]\n",
    "vertical95=[]\n",
    "headline95=[]\n",
    "description95=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date95.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author95.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical95.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline95.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description95.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9f3ce725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df95=pd.DataFrame({})\n",
    "df95['Date']=date95[:1]\n",
    "df95['Author']=author95[:1]\n",
    "df95['Vertical']=vertical95[:1]\n",
    "df95['Healines']=headline95[:1]\n",
    "df95['Description']=description95[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3974bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0fc1da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "date96=[]\n",
    "author96=[]\n",
    "vertical96=[]\n",
    "headline96=[]\n",
    "description96=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date96.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author96.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical96.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline96.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description96.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "dbb8d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df96=pd.DataFrame({})\n",
    "df96['Date']=date96[:1]\n",
    "df96['Author']=author96[:1]\n",
    "df96['Vertical']=vertical96[:1]\n",
    "df96['Healines']=headline96[:1]\n",
    "df96['Description']=description96[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc4485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c5783c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date97=[]\n",
    "author97=[]\n",
    "vertical97=[]\n",
    "headline97=[]\n",
    "description97=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date97.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author97.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical97.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline97.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description97.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9e533134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df97=pd.DataFrame({})\n",
    "df97['Date']=date97[:1]\n",
    "df97['Author']=author97[:1]\n",
    "df97['Vertical']=vertical97[:1]\n",
    "df97['Healines']=headline97[:1]\n",
    "df97['Description']=description97[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac2dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e26540a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date98=[]\n",
    "author98=[]\n",
    "vertical98=[]\n",
    "headline98=[]\n",
    "description98=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date98.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author98.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical98.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline98.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description98.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "353c52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df98=pd.DataFrame({})\n",
    "df98['Date']=date98[:1]\n",
    "df98['Author']=author98[:1]\n",
    "df98['Vertical']=vertical98[:1]\n",
    "df98['Healines']=headline98[:1]\n",
    "df98['Description']=description98[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b1fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b5cb958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date99=[]\n",
    "author99=[]\n",
    "vertical99=[]\n",
    "headline99=[]\n",
    "description99=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date99.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author99.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical99.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline99.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description99.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bfdaf5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df99=pd.DataFrame({})\n",
    "df99['Date']=date99[:1]\n",
    "df99['Author']=author99[:1]\n",
    "df99['Vertical']=vertical99[:1]\n",
    "df99['Healines']=headline99[:1]\n",
    "df99['Description']=description99[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8509ffd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "575c2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "date100=[]\n",
    "author100=[]\n",
    "vertical100=[]\n",
    "headline100=[]\n",
    "description100=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date100.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author100.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical100.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline100.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description100.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d5e3d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df100=pd.DataFrame({})\n",
    "df100['Date']=date100[:1]\n",
    "df100['Author']=author100[:1]\n",
    "df100['Vertical']=vertical100[:1]\n",
    "df100['Healines']=headline100[:1]\n",
    "df100['Description']=description100[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bad869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a229d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date101=[]\n",
    "author101=[]\n",
    "vertical101=[]\n",
    "headline101=[]\n",
    "description101=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date101.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author101.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical101.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline101.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description101.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d1d97ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df101=pd.DataFrame({})\n",
    "df101['Date']=date101[:1]\n",
    "df101['Author']=author101[:1]\n",
    "df101['Vertical']=vertical101[:1]\n",
    "df101['Healines']=headline101[:1]\n",
    "df101['Description']=description101[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa01e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "1a819d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "date102=[]\n",
    "author102=[]\n",
    "vertical102=[]\n",
    "headline102=[]\n",
    "description102=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date102.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author102.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical102.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline102.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description102.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b5efebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df102=pd.DataFrame({})\n",
    "df102['Date']=date102[:1]\n",
    "df102['Author']=author102[:1]\n",
    "df102['Vertical']=vertical102[:1]\n",
    "df102['Healines']=headline102[:1]\n",
    "df102['Description']=description102[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95aabcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1ad77dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date103=[]\n",
    "author103=[]\n",
    "vertical103=[]\n",
    "headline103=[]\n",
    "description103=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date103.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author103.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical103.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline103.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description103.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "7c61145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df103=pd.DataFrame({})\n",
    "df103['Date']=date103[:1]\n",
    "df103['Author']=author103[:1]\n",
    "df103['Vertical']=vertical103[:1]\n",
    "df103['Healines']=headline103[:1]\n",
    "df103['Description']=description103[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babbd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "26317477",
   "metadata": {},
   "outputs": [],
   "source": [
    "date104=[]\n",
    "author104=[]\n",
    "vertical104=[]\n",
    "headline104=[]\n",
    "description104=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date104.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author104.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical104.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline104.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description104.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6f11d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df104=pd.DataFrame({})\n",
    "df104['Date']=date104[:1]\n",
    "df104['Author']=author104[:1]\n",
    "df104['Vertical']=vertical104[:1]\n",
    "df104['Healines']=headline104[:1]\n",
    "df104['Description']=description104[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e04de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "cdc1f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "date105=[]\n",
    "author105=[]\n",
    "vertical105=[]\n",
    "headline105=[]\n",
    "description105=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date105.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author105.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical105.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline105.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description105.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ad096036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df105=pd.DataFrame({})\n",
    "df105['Date']=date105[:1]\n",
    "df105['Author']=author105[:1]\n",
    "df105['Vertical']=vertical105[:1]\n",
    "df105['Healines']=headline105[:1]\n",
    "df105['Description']=description105[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12c5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7cbf98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "date106=[]\n",
    "author106=[]\n",
    "vertical106=[]\n",
    "headline106=[]\n",
    "description106=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date106.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author106.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical106.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline106.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description106.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ef64f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "df106=pd.DataFrame({})\n",
    "df106['Date']=date106[:1]\n",
    "df106['Author']=author106[:1]\n",
    "df106['Vertical']=vertical106[:1]\n",
    "df106['Healines']=headline106[:1]\n",
    "df106['Description']=description106[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35011f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e0a72265",
   "metadata": {},
   "outputs": [],
   "source": [
    "date107=[]\n",
    "author107=[]\n",
    "vertical107=[]\n",
    "headline107=[]\n",
    "description107=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date107.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author107.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical107.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline107.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description107.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b0ea8146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df107=pd.DataFrame({})\n",
    "df107['Date']=date107[:1]\n",
    "df107['Author']=author107[:1]\n",
    "df107['Vertical']=vertical107[:1]\n",
    "df107['Healines']=headline107[:1]\n",
    "df107['Description']=description107[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeadfb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "dc83df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "date108=[]\n",
    "author108=[]\n",
    "vertical108=[]\n",
    "headline108=[]\n",
    "description108=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date108.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author108.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical108.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline108.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description108.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "10d7a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df108=pd.DataFrame({})\n",
    "df108['Date']=date108[:1]\n",
    "df108['Author']=author108[:1]\n",
    "df108['Vertical']=vertical108[:1]\n",
    "df108['Healines']=headline108[:1]\n",
    "df108['Description']=description108[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bf12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "67e78941",
   "metadata": {},
   "outputs": [],
   "source": [
    "date109=[]\n",
    "author109=[]\n",
    "vertical109=[]\n",
    "headline109=[]\n",
    "description109=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date109.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author109.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical109.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline109.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description109.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "aca877ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df109=pd.DataFrame({})\n",
    "df109['Date']=date109[:1]\n",
    "df109['Author']=author109[:1]\n",
    "df109['Vertical']=vertical109[:1]\n",
    "df109['Healines']=headline109[:1]\n",
    "df109['Description']=description109[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777ae32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ba221690",
   "metadata": {},
   "outputs": [],
   "source": [
    "date110=[]\n",
    "author110=[]\n",
    "vertical110=[]\n",
    "headline110=[]\n",
    "description110=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date110.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author110.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical110.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline110.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description110.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "cd19e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df110=pd.DataFrame({})\n",
    "df110['Date']=date110[:1]\n",
    "df110['Author']=author110[:1]\n",
    "df110['Vertical']=vertical110[:1]\n",
    "df110['Healines']=headline110[:1]\n",
    "df110['Description']=description110[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7aa84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "80ed22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date111=[]\n",
    "author111=[]\n",
    "vertical111=[]\n",
    "headline111=[]\n",
    "description111=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date111.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author111.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical111.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline111.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description111.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2f69976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df111=pd.DataFrame({})\n",
    "df111['Date']=date111[:1]\n",
    "df111['Author']=author111[:1]\n",
    "df111['Vertical']=vertical111[:1]\n",
    "df111['Healines']=headline111[:1]\n",
    "df111['Description']=description111[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7cac7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "5111f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "date112=[]\n",
    "author112=[]\n",
    "vertical112=[]\n",
    "headline112=[]\n",
    "description112=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date112.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author112.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical112.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline112.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description112.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "8b5357b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df112=pd.DataFrame({})\n",
    "df112['Date']=date112[:1]\n",
    "df112['Author']=author112[:1]\n",
    "df112['Vertical']=vertical112[:1]\n",
    "df112['Healines']=headline112[:1]\n",
    "df112['Description']=description112[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf4b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "194fe52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date113=[]\n",
    "author113=[]\n",
    "vertical113=[]\n",
    "headline113=[]\n",
    "description113=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date113.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author113.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical113.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline113.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description113.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9006340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df113=pd.DataFrame({})\n",
    "df113['Date']=date113[:1]\n",
    "df113['Author']=author113[:1]\n",
    "df113['Vertical']=vertical113[:1]\n",
    "df113['Healines']=headline113[:1]\n",
    "df113['Description']=description113[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd1437c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "13955fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "date114=[]\n",
    "author114=[]\n",
    "vertical114=[]\n",
    "headline114=[]\n",
    "description114=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date114.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author114.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical114.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline114.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description114.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e4ac5417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df114=pd.DataFrame({})\n",
    "df114['Date']=date114[:1]\n",
    "df114['Author']=author114[:1]\n",
    "df114['Vertical']=vertical114[:1]\n",
    "df114['Healines']=headline114[:1]\n",
    "df114['Description']=description114[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76834386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "a4e0cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date115=[]\n",
    "author115=[]\n",
    "vertical115=[]\n",
    "headline115=[]\n",
    "description115=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date115.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author115.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical115.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline115.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description115.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e23908af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df115=pd.DataFrame({})\n",
    "df115['Date']=date115[:1]\n",
    "df115['Author']=author115[:1]\n",
    "df115['Vertical']=vertical115[:1]\n",
    "df115['Healines']=headline115[:1]\n",
    "df115['Description']=description115[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185ac59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "97c51b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "date116=[]\n",
    "author116=[]\n",
    "vertical116=[]\n",
    "headline116=[]\n",
    "description116=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date116.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author116.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical116.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline116.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description116.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7628f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df116=pd.DataFrame({})\n",
    "df116['Date']=date116[:1]\n",
    "df116['Author']=author116[:1]\n",
    "df116['Vertical']=vertical116[:1]\n",
    "df116['Healines']=headline116[:1]\n",
    "df116['Description']=description116[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596abba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "77780e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date117=[]\n",
    "author117=[]\n",
    "vertical117=[]\n",
    "headline117=[]\n",
    "description117=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date117.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author117.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical117.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline117.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description117.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "bcd96557",
   "metadata": {},
   "outputs": [],
   "source": [
    "df117=pd.DataFrame({})\n",
    "df117['Date']=date117[:1]\n",
    "df117['Author']=author117[:1]\n",
    "df117['Vertical']=vertical117[:1]\n",
    "df117['Healines']=headline117[:1]\n",
    "df117['Description']=description117[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c04b5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3593d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "date118=[]\n",
    "author118=[]\n",
    "vertical118=[]\n",
    "headline118=[]\n",
    "description118=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date118.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author118.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical118.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline118.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description118.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e57794e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df118=pd.DataFrame({})\n",
    "df118['Date']=date118[:1]\n",
    "df118['Author']=author118[:1]\n",
    "df118['Vertical']=vertical118[:1]\n",
    "df118['Healines']=headline118[:1]\n",
    "df118['Description']=description118[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d309a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "00bf7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "date119=[]\n",
    "author119=[]\n",
    "vertical119=[]\n",
    "headline119=[]\n",
    "description119=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date119.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author119.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical119.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline119.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description119.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "0cad43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df119=pd.DataFrame({})\n",
    "df119['Date']=date119[:1]\n",
    "df119['Author']=author119[:1]\n",
    "df119['Vertical']=vertical119[:1]\n",
    "df119['Healines']=headline119[:1]\n",
    "df119['Description']=description119[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bb513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f0c85e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "date120=[]\n",
    "author120=[]\n",
    "vertical120=[]\n",
    "headline120=[]\n",
    "description120=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date120.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author120.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical120.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline120.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description120.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "4b52c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df120=pd.DataFrame({})\n",
    "df120['Date']=date120[:1]\n",
    "df120['Author']=author120[:1]\n",
    "df120['Vertical']=vertical120[:1]\n",
    "df120['Healines']=headline120[:1]\n",
    "df120['Description']=description120[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f484c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "1624f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "date121=[]\n",
    "author121=[]\n",
    "vertical121=[]\n",
    "headline121=[]\n",
    "description121=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date121.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author121.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical121.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline121.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description121.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "780ef72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df121=pd.DataFrame({})\n",
    "df121['Date']=date121[:1]\n",
    "df121['Author']=author121[:1]\n",
    "df121['Vertical']=vertical121[:1]\n",
    "df121['Healines']=headline121[:1]\n",
    "df121['Description']=description121[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2b9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "8868e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "date122=[]\n",
    "author122=[]\n",
    "vertical122=[]\n",
    "headline122=[]\n",
    "description122=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date122.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author122.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical122.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline122.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description122.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "7f7769a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df122=pd.DataFrame({})\n",
    "df122['Date']=date122[:1]\n",
    "df122['Author']=author122[:1]\n",
    "df122['Vertical']=vertical122[:1]\n",
    "df122['Healines']=headline122[:1]\n",
    "df122['Description']=description122[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8ee7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "4c4529e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date123=[]\n",
    "author123=[]\n",
    "vertical123=[]\n",
    "headline123=[]\n",
    "description123=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date123.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author123.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical123.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline123.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description123.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "613a355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df123=pd.DataFrame({})\n",
    "df123['Date']=date123[:1]\n",
    "df123['Author']=author123[:1]\n",
    "df123['Vertical']=vertical123[:1]\n",
    "df123['Healines']=headline123[:1]\n",
    "df123['Description']=description123[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3cea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c2ff8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "date124=[]\n",
    "author124=[]\n",
    "vertical124=[]\n",
    "headline124=[]\n",
    "description124=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date124.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author124.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical124.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline124.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description124.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "60adccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df124=pd.DataFrame({})\n",
    "df124['Date']=date124[:1]\n",
    "df124['Author']=author124[:1]\n",
    "df124['Vertical']=vertical124[:1]\n",
    "df124['Healines']=headline124[:1]\n",
    "df124['Description']=description124[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfed3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "476ce182",
   "metadata": {},
   "outputs": [],
   "source": [
    "date125=[]\n",
    "author125=[]\n",
    "vertical125=[]\n",
    "headline125=[]\n",
    "description125=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date125.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author125.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical125.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline125.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description125.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "7c039f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df125=pd.DataFrame({})\n",
    "df125['Date']=date125[:1]\n",
    "df125['Author']=author125[:1]\n",
    "df125['Vertical']=vertical125[:1]\n",
    "df125['Healines']=headline125[:1]\n",
    "df125['Description']=description125[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454d90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2e308ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date126=[]\n",
    "author126=[]\n",
    "vertical126=[]\n",
    "headline126=[]\n",
    "description126=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date126.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author126.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical126.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline126.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description126.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "36428be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df126=pd.DataFrame({})\n",
    "df126['Date']=date126[:1]\n",
    "df126['Author']=author126[:1]\n",
    "df126['Vertical']=vertical126[:1]\n",
    "df126['Healines']=headline126[:1]\n",
    "df126['Description']=description126[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8063d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "ea164889",
   "metadata": {},
   "outputs": [],
   "source": [
    "date127=[]\n",
    "author127=[]\n",
    "vertical127=[]\n",
    "headline127=[]\n",
    "description127=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date127.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author127.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical127.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline127.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description127.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3f76b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df127=pd.DataFrame({})\n",
    "df127['Date']=date127[:1]\n",
    "df127['Author']=author127[:1]\n",
    "df127['Vertical']=vertical127[:1]\n",
    "df127['Healines']=headline127[:1]\n",
    "df127['Description']=description127[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f48ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "dd6eb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "date128=[]\n",
    "author128=[]\n",
    "vertical128=[]\n",
    "headline128=[]\n",
    "description128=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date128.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author128.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical128.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline128.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description128.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "48627692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df128=pd.DataFrame({})\n",
    "df128['Date']=date128[:1]\n",
    "df128['Author']=author128[:1]\n",
    "df128['Vertical']=vertical128[:1]\n",
    "df128['Healines']=headline128[:1]\n",
    "df128['Description']=description128[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40dd98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "d64bebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date129=[]\n",
    "author129=[]\n",
    "vertical129=[]\n",
    "headline129=[]\n",
    "description129=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date129.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author129.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical129.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline129.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description129.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "b2c8d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df129=pd.DataFrame({})\n",
    "df129['Date']=date129[:1]\n",
    "df129['Author']=author129[:1]\n",
    "df129['Vertical']=vertical129[:1]\n",
    "df129['Healines']=headline129[:1]\n",
    "df129['Description']=description129[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f36fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "57e214d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "date130=[]\n",
    "author130=[]\n",
    "vertical130=[]\n",
    "headline130=[]\n",
    "description130=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date130.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author130.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical130.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline130.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description130.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "d6503af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df130=pd.DataFrame({})\n",
    "df130['Date']=date130[:1]\n",
    "df130['Author']=author130[:1]\n",
    "df130['Vertical']=vertical130[:1]\n",
    "df130['Healines']=headline130[:1]\n",
    "df130['Description']=description130[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcff12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f220f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "date131=[]\n",
    "author131=[]\n",
    "vertical131=[]\n",
    "headline131=[]\n",
    "description131=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date131.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author131.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical131.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline131.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description131.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "9cc367f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df131=pd.DataFrame({})\n",
    "df131['Date']=date131[:1]\n",
    "df131['Author']=author131[:1]\n",
    "df131['Vertical']=vertical131[:1]\n",
    "df131['Healines']=headline131[:1]\n",
    "df131['Description']=description131[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96c75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "4271933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date132=[]\n",
    "author132=[]\n",
    "vertical132=[]\n",
    "headline132=[]\n",
    "description132=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date132.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author132.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical132.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline132.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description132.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "71b0ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df132=pd.DataFrame({})\n",
    "df132['Date']=date132[:1]\n",
    "df132['Author']=author132[:1]\n",
    "df132['Vertical']=vertical132[:1]\n",
    "df132['Healines']=headline132[:1]\n",
    "df132['Description']=description132[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea580b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "ddfbabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date133=[]\n",
    "author133=[]\n",
    "vertical133=[]\n",
    "headline133=[]\n",
    "description133=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date133.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author133.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical133.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline133.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description133.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "4e55e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df133=pd.DataFrame({})\n",
    "df133['Date']=date133[:1]\n",
    "df133['Author']=author133[:1]\n",
    "df133['Vertical']=vertical133[:1]\n",
    "df133['Healines']=headline133[:1]\n",
    "df133['Description']=description133[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bdc513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "0d87bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date134=[]\n",
    "author134=[]\n",
    "vertical134=[]\n",
    "headline134=[]\n",
    "description134=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date134.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author134.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical134.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline134.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description134.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "41ee1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df134=pd.DataFrame({})\n",
    "df134['Date']=date134[:1]\n",
    "df134['Author']=author134[:1]\n",
    "df134['Vertical']=vertical134[:1]\n",
    "df134['Healines']=headline134[:1]\n",
    "df134['Description']=description134[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e19efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "02cff05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "date135=[]\n",
    "author135=[]\n",
    "vertical135=[]\n",
    "headline135=[]\n",
    "description135=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date135.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author135.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical135.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline135.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description135.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "8a1599a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df135=pd.DataFrame({})\n",
    "df135['Date']=date135[:1]\n",
    "df135['Author']=author135[:1]\n",
    "df135['Vertical']=vertical135[:1]\n",
    "df135['Healines']=headline135[:1]\n",
    "df135['Description']=description135[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3dc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "0eeaee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "date136=[]\n",
    "author136=[]\n",
    "vertical136=[]\n",
    "headline136=[]\n",
    "description136=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date136.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author136.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical136.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline136.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description136.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "6f4d5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df136=pd.DataFrame({})\n",
    "df136['Date']=date136[:1]\n",
    "df136['Author']=author136[:1]\n",
    "df136['Vertical']=vertical136[:1]\n",
    "df136['Healines']=headline136[:1]\n",
    "df136['Description']=description136[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b97592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "0e5126d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date137=[]\n",
    "author137=[]\n",
    "vertical137=[]\n",
    "headline137=[]\n",
    "description137=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date137.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author137.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical137.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline137.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description137.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "35ea8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df137=pd.DataFrame({})\n",
    "df137['Date']=date137[:1]\n",
    "df137['Author']=author137[:1]\n",
    "df137['Vertical']=vertical137[:1]\n",
    "df137['Healines']=headline137[:1]\n",
    "df137['Description']=description137[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2489e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d39942be",
   "metadata": {},
   "outputs": [],
   "source": [
    "date138=[]\n",
    "author138=[]\n",
    "vertical138=[]\n",
    "headline138=[]\n",
    "description138=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date138.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author138.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical138.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline138.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description138.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "6cca6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df138=pd.DataFrame({})\n",
    "df138['Date']=date138[:1]\n",
    "df138['Author']=author138[:1]\n",
    "df138['Vertical']=vertical138[:1]\n",
    "df138['Healines']=headline138[:1]\n",
    "df138['Description']=description138[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89579d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "7a2c9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "date139=[]\n",
    "author139=[]\n",
    "vertical139=[]\n",
    "headline139=[]\n",
    "description139=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date139.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author139.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical139.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline139.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description139.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f5a8b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df139=pd.DataFrame({})\n",
    "df139['Date']=date139[:1]\n",
    "df139['Author']=author139[:1]\n",
    "df139['Vertical']=vertical139[:1]\n",
    "df139['Healines']=headline139[:1]\n",
    "df139['Description']=description139[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb1b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "ae075ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date140=[]\n",
    "author140=[]\n",
    "vertical140=[]\n",
    "headline140=[]\n",
    "description140=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date140.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author140.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical140.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline140.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description140.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "aff77b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df140=pd.DataFrame({})\n",
    "df140['Date']=date140[:1]\n",
    "df140['Author']=author140[:1]\n",
    "df140['Vertical']=vertical140[:1]\n",
    "df140['Healines']=headline140[:1]\n",
    "df140['Description']=description140[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba5e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "d87bb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "date141=[]\n",
    "author141=[]\n",
    "vertical141=[]\n",
    "headline141=[]\n",
    "description141=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date141.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author141.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical141.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline141.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description141.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "a8bbfb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df141=pd.DataFrame({})\n",
    "df141['Date']=date141[:1]\n",
    "df141['Author']=author141[:1]\n",
    "df141['Vertical']=vertical141[:1]\n",
    "df141['Healines']=headline141[:1]\n",
    "df141['Description']=description141[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b403e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "66df4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "date142=[]\n",
    "author142=[]\n",
    "vertical142=[]\n",
    "headline142=[]\n",
    "description142=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date142.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author142.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical142.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline142.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description142.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "f77b2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df142=pd.DataFrame({})\n",
    "df142['Date']=date142[:1]\n",
    "df142['Author']=author142[:1]\n",
    "df142['Vertical']=vertical142[:1]\n",
    "df142['Healines']=headline142[:1]\n",
    "df142['Description']=description142[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58946f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "3a394246",
   "metadata": {},
   "outputs": [],
   "source": [
    "date143=[]\n",
    "author143=[]\n",
    "vertical143=[]\n",
    "headline143=[]\n",
    "description143=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date143.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author143.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical143.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline143.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description143.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "51244645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df143=pd.DataFrame({})\n",
    "df143['Date']=date143[:1]\n",
    "df143['Author']=author143[:1]\n",
    "df143['Vertical']=vertical143[:1]\n",
    "df143['Healines']=headline143[:1]\n",
    "df143['Description']=description143[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981556a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "ff4f5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "date144=[]\n",
    "author144=[]\n",
    "vertical144=[]\n",
    "headline144=[]\n",
    "description144=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date144.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author144.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical144.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline144.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description144.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "2a40a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df144=pd.DataFrame({})\n",
    "df144['Date']=date144[:1]\n",
    "df144['Author']=author144[:1]\n",
    "df144['Vertical']=vertical144[:1]\n",
    "df144['Healines']=headline144[:1]\n",
    "df144['Description']=description144[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc476c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "8cbd016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date145=[]\n",
    "author145=[]\n",
    "vertical145=[]\n",
    "headline145=[]\n",
    "description145=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date145.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author145.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical145.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline145.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description145.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "7a48fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df145=pd.DataFrame({})\n",
    "df145['Date']=date145[:1]\n",
    "df145['Author']=author145[:1]\n",
    "df145['Vertical']=vertical145[:1]\n",
    "df145['Healines']=headline145[:1]\n",
    "df145['Description']=description145[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7612a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "d8612b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "date146=[]\n",
    "author146=[]\n",
    "vertical146=[]\n",
    "headline146=[]\n",
    "description146=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date146.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author146.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical146.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline146.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description146.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "67eaac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df146=pd.DataFrame({})\n",
    "df146['Date']=date146[:1]\n",
    "df146['Author']=author146[:1]\n",
    "df146['Vertical']=vertical146[:1]\n",
    "df146['Healines']=headline146[:1]\n",
    "df146['Description']=description146[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063b084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "702e347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date147=[]\n",
    "author147=[]\n",
    "vertical147=[]\n",
    "headline147=[]\n",
    "description147=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date147.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author147.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical147.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline147.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description147.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "76907bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df147=pd.DataFrame({})\n",
    "df147['Date']=date147[:1]\n",
    "df147['Author']=author147[:1]\n",
    "df147['Vertical']=vertical147[:1]\n",
    "df147['Healines']=headline147[:1]\n",
    "df147['Description']=description147[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5999f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "f61eddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date148=[]\n",
    "author148=[]\n",
    "vertical148=[]\n",
    "headline148=[]\n",
    "description148=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date148.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author148.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical148.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline148.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description148.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "6bd0b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df148=pd.DataFrame({})\n",
    "df148['Date']=date148[:1]\n",
    "df148['Author']=author148[:1]\n",
    "df148['Vertical']=vertical148[:1]\n",
    "df148['Healines']=headline148[:1]\n",
    "df148['Description']=description148[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391b3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "c9f5afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "date149=[]\n",
    "author149=[]\n",
    "vertical149=[]\n",
    "headline149=[]\n",
    "description149=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date149.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author149.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical149.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline149.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description149.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "085a26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df149=pd.DataFrame({})\n",
    "df149['Date']=date149[:1]\n",
    "df149['Author']=author149[:1]\n",
    "df149['Vertical']=vertical149[:1]\n",
    "df149['Healines']=headline149[:1]\n",
    "df149['Description']=description149[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72751d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "01993cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "date150=[]\n",
    "author150=[]\n",
    "vertical150=[]\n",
    "headline150=[]\n",
    "description150=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date150.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author150.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical150.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline150.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description150.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "fbff2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df150=pd.DataFrame({})\n",
    "df150['Date']=date150[:1]\n",
    "df150['Author']=author150[:1]\n",
    "df150['Vertical']=vertical150[:1]\n",
    "df150['Healines']=headline150[:1]\n",
    "df150['Description']=description150[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5f8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "aeff9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "date151=[]\n",
    "author151=[]\n",
    "vertical151=[]\n",
    "headline151=[]\n",
    "description151=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date151.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author151.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical151.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline151.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description151.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "19aa8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "df151=pd.DataFrame({})\n",
    "df151['Date']=date151[:1]\n",
    "df151['Author']=author151[:1]\n",
    "df151['Vertical']=vertical151[:1]\n",
    "df151['Healines']=headline151[:1]\n",
    "df151['Description']=description151[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b91dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "5215aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "date152=[]\n",
    "author152=[]\n",
    "vertical152=[]\n",
    "headline152=[]\n",
    "description152=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date152.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author152.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical152.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline152.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description152.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "9f90684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df152=pd.DataFrame({})\n",
    "df152['Date']=date152[:1]\n",
    "df152['Author']=author152[:1]\n",
    "df152['Vertical']=vertical152[:1]\n",
    "df152['Healines']=headline152[:1]\n",
    "df152['Description']=description152[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e0fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "ec56757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date153=[]\n",
    "author153=[]\n",
    "vertical153=[]\n",
    "headline153=[]\n",
    "description153=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date153.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author153.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical153.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline153.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description153.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5ffcd2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df153=pd.DataFrame({})\n",
    "df153['Date']=date153[:1]\n",
    "df153['Author']=author153[:1]\n",
    "df153['Vertical']=vertical153[:1]\n",
    "df153['Healines']=headline153[:1]\n",
    "df153['Description']=description153[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063bdd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c3bd4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date154=[]\n",
    "author154=[]\n",
    "vertical154=[]\n",
    "headline154=[]\n",
    "description154=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date154.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author154.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical154.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline154.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description154.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "cc008401",
   "metadata": {},
   "outputs": [],
   "source": [
    "df154=pd.DataFrame({})\n",
    "df154['Date']=date154[:1]\n",
    "df154['Author']=author154[:1]\n",
    "df154['Vertical']=vertical154[:1]\n",
    "df154['Healines']=headline154[:1]\n",
    "df154['Description']=description154[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c4a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "bc86e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "date155=[]\n",
    "author155=[]\n",
    "vertical155=[]\n",
    "headline155=[]\n",
    "description155=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date155.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author155.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical155.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline155.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description155.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "a38b2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df155=pd.DataFrame({})\n",
    "df155['Date']=date155[:1]\n",
    "df155['Author']=author155[:1]\n",
    "df155['Vertical']=vertical155[:1]\n",
    "df155['Healines']=headline155[:1]\n",
    "df155['Description']=description155[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3596a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "3090c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date156=[]\n",
    "author156=[]\n",
    "vertical156=[]\n",
    "headline156=[]\n",
    "description156=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date156.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author156.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical156.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline156.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description156.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "09959831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df156=pd.DataFrame({})\n",
    "df156['Date']=date156[:1]\n",
    "df156['Author']=author156[:1]\n",
    "df156['Vertical']=vertical156[:1]\n",
    "df156['Healines']=headline156[:1]\n",
    "df156['Description']=description156[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30e7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "38ab32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "date157=[]\n",
    "author157=[]\n",
    "vertical157=[]\n",
    "headline157=[]\n",
    "description157=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date157.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author157.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical157.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline157.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description157.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "64822b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df157=pd.DataFrame({})\n",
    "df157['Date']=date157[:1]\n",
    "df157['Author']=author157[:1]\n",
    "df157['Vertical']=vertical157[:1]\n",
    "df157['Healines']=headline157[:1]\n",
    "df157['Description']=description157[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2588f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "c42f2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "date158=[]\n",
    "author158=[]\n",
    "vertical158=[]\n",
    "headline158=[]\n",
    "description158=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date158.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author158.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical158.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline158.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description158.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "b6fc6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df158=pd.DataFrame({})\n",
    "df158['Date']=date158[:1]\n",
    "df158['Author']=author158[:1]\n",
    "df158['Vertical']=vertical158[:1]\n",
    "df158['Healines']=headline158[:1]\n",
    "df158['Description']=description158[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e234b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "ec85623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date159=[]\n",
    "author159=[]\n",
    "vertical159=[]\n",
    "headline159=[]\n",
    "description159=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date159.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author159.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical159.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline159.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description159.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "550e9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df159=pd.DataFrame({})\n",
    "df159['Date']=date159[:1]\n",
    "df159['Author']=author159[:1]\n",
    "df159['Vertical']=vertical159[:1]\n",
    "df159['Healines']=headline159[:1]\n",
    "df159['Description']=description159[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca762227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "7f8dd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "date160=[]\n",
    "author160=[]\n",
    "vertical160=[]\n",
    "headline160=[]\n",
    "description160=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date160.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author160.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical160.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline160.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description160.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1f5f6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df160=pd.DataFrame({})\n",
    "df160['Date']=date160[:1]\n",
    "df160['Author']=author160[:1]\n",
    "df160['Vertical']=vertical160[:1]\n",
    "df160['Healines']=headline160[:1]\n",
    "df160['Description']=description160[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc5f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "a56d60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date161=[]\n",
    "author161=[]\n",
    "vertical161=[]\n",
    "headline161=[]\n",
    "description161=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date161.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author161.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical161.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline161.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description161.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "368d8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df161=pd.DataFrame({})\n",
    "df161['Date']=date161[:1]\n",
    "df161['Author']=author161[:1]\n",
    "df161['Vertical']=vertical161[:1]\n",
    "df161['Healines']=headline161[:1]\n",
    "df161['Description']=description161[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d2dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "cb94f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date162=[]\n",
    "author162=[]\n",
    "vertical162=[]\n",
    "headline162=[]\n",
    "description162=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date162.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author162.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical162.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline162.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description162.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b1b770b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df162=pd.DataFrame({})\n",
    "df162['Date']=date162[:1]\n",
    "df162['Author']=author162[:1]\n",
    "df162['Vertical']=vertical162[:1]\n",
    "df162['Healines']=headline162[:1]\n",
    "df162['Description']=description162[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b004770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "e0b1991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "date163=[]\n",
    "author163=[]\n",
    "vertical163=[]\n",
    "headline163=[]\n",
    "description163=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date163.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author163.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical163.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline163.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description163.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "cef2b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df163=pd.DataFrame({})\n",
    "df163['Date']=date163[:1]\n",
    "df163['Author']=author163[:1]\n",
    "df163['Vertical']=vertical163[:1]\n",
    "df163['Healines']=headline163[:1]\n",
    "df163['Description']=description163[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d340ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "13b46cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "date164=[]\n",
    "author164=[]\n",
    "vertical164=[]\n",
    "headline164=[]\n",
    "description164=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date164.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author164.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical164.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline164.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description164.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "707ff24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df164=pd.DataFrame({})\n",
    "df164['Date']=date164[:1]\n",
    "df164['Author']=author164[:1]\n",
    "df164['Vertical']=vertical164[:1]\n",
    "df164['Healines']=headline164[:1]\n",
    "df164['Description']=description164[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213e6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a8c8c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "date165=[]\n",
    "author165=[]\n",
    "vertical165=[]\n",
    "headline165=[]\n",
    "description165=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date165.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author165.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical165.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline165.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description165.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "fdfe249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df165=pd.DataFrame({})\n",
    "df165['Date']=date165[:1]\n",
    "df165['Author']=author165[:1]\n",
    "df165['Vertical']=vertical165[:1]\n",
    "df165['Healines']=headline165[:1]\n",
    "df165['Description']=description165[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fee92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "837c8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "date166=[]\n",
    "author166=[]\n",
    "vertical166=[]\n",
    "headline166=[]\n",
    "description166=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date166.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author166.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical166.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline166.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description166.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "caa27928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df166=pd.DataFrame({})\n",
    "df166['Date']=date166[:1]\n",
    "df166['Author']=author166[:1]\n",
    "df166['Vertical']=vertical166[:1]\n",
    "df166['Healines']=headline166[:1]\n",
    "df166['Description']=description166[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aee972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "ece1180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date167=[]\n",
    "author167=[]\n",
    "vertical167=[]\n",
    "headline167=[]\n",
    "description167=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date167.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author167.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical167.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline167.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description167.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "251e1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df167=pd.DataFrame({})\n",
    "df167['Date']=date167[:1]\n",
    "df167['Author']=author167[:1]\n",
    "df167['Vertical']=vertical167[:1]\n",
    "df167['Healines']=headline167[:1]\n",
    "df167['Description']=description167[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903e53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "90cf0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "date168=[]\n",
    "author168=[]\n",
    "vertical168=[]\n",
    "headline168=[]\n",
    "description168=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date168.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author168.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical168.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline168.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description168.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "da19fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df168=pd.DataFrame({})\n",
    "df168['Date']=date168[:1]\n",
    "df168['Author']=author168[:1]\n",
    "df168['Vertical']=vertical168[:1]\n",
    "df168['Healines']=headline168[:1]\n",
    "df168['Description']=description168[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680e9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "d45cbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "date169=[]\n",
    "author169=[]\n",
    "vertical169=[]\n",
    "headline169=[]\n",
    "description169=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date169.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author169.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical169.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline169.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description169.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "1be0992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df169=pd.DataFrame({})\n",
    "df169['Date']=date169[:1]\n",
    "df169['Author']=author169[:1]\n",
    "df169['Vertical']=vertical169[:1]\n",
    "df169['Healines']=headline169[:1]\n",
    "df169['Description']=description169[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd2539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4e66b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date170=[]\n",
    "author170=[]\n",
    "vertical170=[]\n",
    "headline170=[]\n",
    "description170=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date170.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author170.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical170.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline170.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description170.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "a0dd3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df170=pd.DataFrame({})\n",
    "df170['Date']=date170[:1]\n",
    "df170['Author']=author170[:1]\n",
    "df170['Vertical']=vertical170[:1]\n",
    "df170['Healines']=headline170[:1]\n",
    "df170['Description']=description170[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85535ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "8c248fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "date171=[]\n",
    "author171=[]\n",
    "vertical171=[]\n",
    "headline171=[]\n",
    "description171=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date171.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author171.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical171.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline171.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description171.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "ebcc9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df171=pd.DataFrame({})\n",
    "df171['Date']=date171[:1]\n",
    "df171['Author']=author171[:1]\n",
    "df171['Vertical']=vertical171[:1]\n",
    "df171['Healines']=headline171[:1]\n",
    "df171['Description']=description171[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a9cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "48d8066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date172=[]\n",
    "author172=[]\n",
    "vertical172=[]\n",
    "headline172=[]\n",
    "description172=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date172.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author172.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical172.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline172.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description172.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "3784b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df172=pd.DataFrame({})\n",
    "df172['Date']=date172[:1]\n",
    "df172['Author']=author172[:1]\n",
    "df172['Vertical']=vertical172[:1]\n",
    "df172['Healines']=headline172[:1]\n",
    "df172['Description']=description172[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7167f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "decb6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "date173=[]\n",
    "author173=[]\n",
    "vertical173=[]\n",
    "headline173=[]\n",
    "description173=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date173.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author173.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical173.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline173.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description173.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "e76b864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df173=pd.DataFrame({})\n",
    "df173['Date']=date173[:1]\n",
    "df173['Author']=author173[:1]\n",
    "df173['Vertical']=vertical173[:1]\n",
    "df173['Healines']=headline173[:1]\n",
    "df173['Description']=description173[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a530e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "b989f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date174=[]\n",
    "author174=[]\n",
    "vertical174=[]\n",
    "headline174=[]\n",
    "description174=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date174.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author174.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical174.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline174.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description174.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "2be8fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df174=pd.DataFrame({})\n",
    "df174['Date']=date174[:1]\n",
    "df174['Author']=author174[:1]\n",
    "df174['Vertical']=vertical174[:1]\n",
    "df174['Healines']=headline174[:1]\n",
    "df174['Description']=description174[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b6287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "88741877",
   "metadata": {},
   "outputs": [],
   "source": [
    "date175=[]\n",
    "author175=[]\n",
    "vertical175=[]\n",
    "headline175=[]\n",
    "description175=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date175.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author175.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical175.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline175.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description175.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "f8dc0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df175=pd.DataFrame({})\n",
    "df175['Date']=date175[:1]\n",
    "df175['Author']=author175[:1]\n",
    "df175['Vertical']=vertical175[:1]\n",
    "df175['Healines']=headline175[:1]\n",
    "df175['Description']=description175[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4b7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "b9865abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "date176=[]\n",
    "author176=[]\n",
    "vertical176=[]\n",
    "headline176=[]\n",
    "description176=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date176.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author176.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical176.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline176.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description176.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "1e9cd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df176=pd.DataFrame({})\n",
    "df176['Date']=date176[:1]\n",
    "df176['Author']=author176[:1]\n",
    "df176['Vertical']=vertical176[:1]\n",
    "df176['Healines']=headline176[:1]\n",
    "df176['Description']=description176[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666f6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "53f46cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "date177=[]\n",
    "author177=[]\n",
    "vertical177=[]\n",
    "headline177=[]\n",
    "description177=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date177.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author177.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical177.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline177.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description177.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "f1dc44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df177=pd.DataFrame({})\n",
    "df177['Date']=date177[:1]\n",
    "df177['Author']=author177[:1]\n",
    "df177['Vertical']=vertical177[:1]\n",
    "df177['Healines']=headline177[:1]\n",
    "df177['Description']=description177[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200e2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "efdace9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date178=[]\n",
    "author178=[]\n",
    "vertical178=[]\n",
    "headline178=[]\n",
    "description178=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date178.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author178.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical178.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline178.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description178.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "809a959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df178=pd.DataFrame({})\n",
    "df178['Date']=date178[:1]\n",
    "df178['Author']=author178[:1]\n",
    "df178['Vertical']=vertical178[:1]\n",
    "df178['Healines']=headline178[:1]\n",
    "df178['Description']=description178[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043da607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "b3ce3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "date179=[]\n",
    "author179=[]\n",
    "vertical179=[]\n",
    "headline179=[]\n",
    "description179=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date179.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author179.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical179.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline179.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description179.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "a871114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df179=pd.DataFrame({})\n",
    "df179['Date']=date179[:1]\n",
    "df179['Author']=author179[:1]\n",
    "df179['Vertical']=vertical179[:1]\n",
    "df179['Healines']=headline179[:1]\n",
    "df179['Description']=description179[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5382d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "33767b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date180=[]\n",
    "author180=[]\n",
    "vertical180=[]\n",
    "headline180=[]\n",
    "description180=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date180.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author180.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical180.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline180.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description180.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "fbfbab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df180=pd.DataFrame({})\n",
    "df180['Date']=date180[:1]\n",
    "df180['Author']=author180[:1]\n",
    "df180['Vertical']=vertical180[:1]\n",
    "df180['Healines']=headline180[:1]\n",
    "df180['Description']=description180[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5850a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "f5624d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "date181=[]\n",
    "author181=[]\n",
    "vertical181=[]\n",
    "headline181=[]\n",
    "description181=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date181.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author181.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical181.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline181.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description181.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "84372c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df181=pd.DataFrame({})\n",
    "df181['Date']=date181[:1]\n",
    "df181['Author']=author181[:1]\n",
    "df181['Vertical']=vertical181[:1]\n",
    "df181['Healines']=headline181[:1]\n",
    "df181['Description']=description181[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a4fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "d34727be",
   "metadata": {},
   "outputs": [],
   "source": [
    "date182=[]\n",
    "author182=[]\n",
    "vertical182=[]\n",
    "headline182=[]\n",
    "description182=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date182.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author182.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical182.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline182.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description182.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "53b06a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df182=pd.DataFrame({})\n",
    "df182['Date']=date182[:1]\n",
    "df182['Author']=author182[:1]\n",
    "df182['Vertical']=vertical182[:1]\n",
    "df182['Healines']=headline182[:1]\n",
    "df182['Description']=description182[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102e687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "5c66eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "date183=[]\n",
    "author183=[]\n",
    "vertical183=[]\n",
    "headline183=[]\n",
    "description183=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date183.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author183.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical183.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline183.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description183.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "14375572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df183=pd.DataFrame({})\n",
    "df183['Date']=date183[:1]\n",
    "df183['Author']=author183[:1]\n",
    "df183['Vertical']=vertical183[:1]\n",
    "df183['Healines']=headline183[:1]\n",
    "df183['Description']=description183[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71e097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "9f5953fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "date184=[]\n",
    "author184=[]\n",
    "vertical184=[]\n",
    "headline184=[]\n",
    "description184=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date184.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author184.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical184.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline184.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description184.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "497a9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df184=pd.DataFrame({})\n",
    "df184['Date']=date184[:1]\n",
    "df184['Author']=author184[:1]\n",
    "df184['Vertical']=vertical184[:1]\n",
    "df184['Healines']=headline184[:1]\n",
    "df184['Description']=description184[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5255c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "572c1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "date185=[]\n",
    "author185=[]\n",
    "vertical185=[]\n",
    "headline185=[]\n",
    "description185=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date185.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author185.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical185.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline185.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description185.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "c77e7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df185=pd.DataFrame({})\n",
    "df185['Date']=date185[:1]\n",
    "df185['Author']=author185[:1]\n",
    "df185['Vertical']=vertical185[:1]\n",
    "df185['Healines']=headline185[:1]\n",
    "df185['Description']=description185[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3d407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "0494a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "date186=[]\n",
    "author186=[]\n",
    "vertical186=[]\n",
    "headline186=[]\n",
    "description186=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date186.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author186.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical186.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline186.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description186.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "5ed38dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df186=pd.DataFrame({})\n",
    "df186['Date']=date186[:1]\n",
    "df186['Author']=author186[:1]\n",
    "df186['Vertical']=vertical186[:1]\n",
    "df186['Healines']=headline186[:1]\n",
    "df186['Description']=description186[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1e9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "90790333",
   "metadata": {},
   "outputs": [],
   "source": [
    "date187=[]\n",
    "author187=[]\n",
    "vertical187=[]\n",
    "headline187=[]\n",
    "description187=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date187.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author187.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical187.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline187.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description187.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "498d0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df187=pd.DataFrame({})\n",
    "df187['Date']=date187[:1]\n",
    "df187['Author']=author187[:1]\n",
    "df187['Vertical']=vertical187[:1]\n",
    "df187['Healines']=headline187[:1]\n",
    "df187['Description']=description187[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9e889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "94a19603",
   "metadata": {},
   "outputs": [],
   "source": [
    "date188=[]\n",
    "author188=[]\n",
    "vertical188=[]\n",
    "headline188=[]\n",
    "description188=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date188.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author188.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical188.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline188.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description188.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "7269c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df188=pd.DataFrame({})\n",
    "df188['Date']=date188[:1]\n",
    "df188['Author']=author188[:1]\n",
    "df188['Vertical']=vertical188[:1]\n",
    "df188['Healines']=headline188[:1]\n",
    "df188['Description']=description188[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca75704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "f7bc502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date189=[]\n",
    "author189=[]\n",
    "vertical189=[]\n",
    "headline189=[]\n",
    "description189=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date189.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author189.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical189.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline189.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description189.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "c9df87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df189=pd.DataFrame({})\n",
    "df189['Date']=date189[:1]\n",
    "df189['Author']=author189[:1]\n",
    "df189['Vertical']=vertical189[:1]\n",
    "df189['Healines']=headline189[:1]\n",
    "df189['Description']=description189[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dfee2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "897fed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "date190=[]\n",
    "author190=[]\n",
    "vertical190=[]\n",
    "headline190=[]\n",
    "description190=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date190.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author190.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical190.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline190.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description190.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "d6b59eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df190=pd.DataFrame({})\n",
    "df190['Date']=date190[:1]\n",
    "df190['Author']=author190[:1]\n",
    "df190['Vertical']=vertical190[:1]\n",
    "df190['Healines']=headline190[:1]\n",
    "df190['Description']=description190[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd598ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "1e6b8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date191=[]\n",
    "author191=[]\n",
    "vertical191=[]\n",
    "headline191=[]\n",
    "description191=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date191.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author191.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical191.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline191.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description191.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "faa285c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df191=pd.DataFrame({})\n",
    "df191['Date']=date191[:1]\n",
    "df191['Author']=author191[:1]\n",
    "df191['Vertical']=vertical191[:1]\n",
    "df191['Healines']=headline191[:1]\n",
    "df191['Description']=description191[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd2036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "a399286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date192=[]\n",
    "author192=[]\n",
    "vertical192=[]\n",
    "headline192=[]\n",
    "description192=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date192.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author192.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical192.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline192.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description192.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "745e4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df192=pd.DataFrame({})\n",
    "df192['Date']=date192[:1]\n",
    "df192['Author']=author192[:1]\n",
    "df192['Vertical']=vertical192[:1]\n",
    "df192['Healines']=headline192[:1]\n",
    "df192['Description']=description192[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67230bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "5e64a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "date193=[]\n",
    "author193=[]\n",
    "vertical193=[]\n",
    "headline193=[]\n",
    "description193=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date193.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author193.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical193.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline193.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description193.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "bd8f8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df193=pd.DataFrame({})\n",
    "df193['Date']=date193[:1]\n",
    "df193['Author']=author193[:1]\n",
    "df193['Vertical']=vertical193[:1]\n",
    "df193['Healines']=headline193[:1]\n",
    "df193['Description']=description193[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81083218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "39d6b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date194=[]\n",
    "author194=[]\n",
    "vertical194=[]\n",
    "headline194=[]\n",
    "description194=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date194.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author194.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical194.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline194.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description194.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "bae3f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df194=pd.DataFrame({})\n",
    "df194['Date']=date194[:1]\n",
    "df194['Author']=author194[:1]\n",
    "df194['Vertical']=vertical194[:1]\n",
    "df194['Healines']=headline194[:1]\n",
    "df194['Description']=description194[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b178b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "10570ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date195=[]\n",
    "author195=[]\n",
    "vertical195=[]\n",
    "headline195=[]\n",
    "description195=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date195.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author195.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical195.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline195.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description195.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "450f4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df195=pd.DataFrame({})\n",
    "df195['Date']=date195[:1]\n",
    "df195['Author']=author195[:1]\n",
    "df195['Vertical']=vertical195[:1]\n",
    "df195['Healines']=headline195[:1]\n",
    "df195['Description']=description195[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f420f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "0684e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "date196=[]\n",
    "author196=[]\n",
    "vertical196=[]\n",
    "headline196=[]\n",
    "description196=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date196.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author196.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical196.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline196.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description196.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "6c1e92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df196=pd.DataFrame({})\n",
    "df196['Date']=date196[:1]\n",
    "df196['Author']=author196[:1]\n",
    "df196['Vertical']=vertical196[:1]\n",
    "df196['Healines']=headline196[:1]\n",
    "df196['Description']=description196[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c8784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "c9599dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "date197=[]\n",
    "author197=[]\n",
    "vertical197=[]\n",
    "headline197=[]\n",
    "description197=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date197.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author197.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical197.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline197.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description197.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "1677d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df197=pd.DataFrame({})\n",
    "df197['Date']=date197[:1]\n",
    "df197['Author']=author197[:1]\n",
    "df197['Vertical']=vertical197[:1]\n",
    "df197['Healines']=headline197[:1]\n",
    "df197['Description']=description197[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb103668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "dfda82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date198=[]\n",
    "author198=[]\n",
    "vertical198=[]\n",
    "headline198=[]\n",
    "description198=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date198.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author198.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical198.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline198.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description198.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "f24aa24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df198=pd.DataFrame({})\n",
    "df198['Date']=date198[:1]\n",
    "df198['Author']=author198[:1]\n",
    "df198['Vertical']=vertical198[:1]\n",
    "df198['Healines']=headline198[:1]\n",
    "df198['Description']=description198[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809e03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "45776742",
   "metadata": {},
   "outputs": [],
   "source": [
    "date199=[]\n",
    "author199=[]\n",
    "vertical199=[]\n",
    "headline199=[]\n",
    "description199=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date199.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author199.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical199.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline199.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description199.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "d7913481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df199=pd.DataFrame({})\n",
    "df199['Date']=date199[:1]\n",
    "df199['Author']=author199[:1]\n",
    "df199['Vertical']=vertical199[:1]\n",
    "df199['Healines']=headline199[:1]\n",
    "df199['Description']=description199[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19d5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "e3b9fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "date200=[]\n",
    "author200=[]\n",
    "vertical200=[]\n",
    "headline200=[]\n",
    "description200=[]\n",
    "\n",
    "# scrapping details 31 july 2017\n",
    "\n",
    "dt=driver.find_elements_by_xpath(\"//span[@itemprop='datePublished']\")\n",
    "for i in dt:\n",
    "    date200.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "at=driver.find_elements_by_xpath(\"//span[@itemprop='author']\")\n",
    "for i in at:\n",
    "     author200.append(i.text)\n",
    "   \n",
    "        \n",
    "    \n",
    "ve=driver.find_elements_by_xpath(\"//div[@class='col-12']\")\n",
    "for i in ve:\n",
    "    vertical200.append(i.text)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "hd=driver.find_elements_by_xpath(\"//h2[@itemprop='headline']\")\n",
    "for i in hd:\n",
    "     headline200.append(i.text)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "desc=driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "for i in desc:\n",
    "    description200.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "99f77d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df200=pd.DataFrame({})\n",
    "df200['Date']=date200[:1]\n",
    "df200['Author']=author200[:1]\n",
    "df200['Vertical']=vertical200[:1]\n",
    "df200['Healines']=headline200[:1]\n",
    "df200['Description']=description200[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539f390",
   "metadata": {},
   "source": [
    "# making dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "82f48134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,\n",
    "             df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32,df33,df34,df35,df36,df37,df38,df39,df40,\n",
    "             df41,df42,df43,df44,df45,df46,df47,df48,df49,df50,df51,df52,df53,df54,df55,df56,df57,df58,df59,df60,\n",
    "             df61,df62,df63,df64,df65,df66,df67,df68,df69,df70,df71,df72,df73,df74,df75,df76,df77,df78,df79,df80,\n",
    "             df81,df82,df83,df84,df85,df86,df87,df88,df89,df90,df91,df92,df93,df94,df95,df96,df97,df98,df99,df100,\n",
    "              df101,df102,df103,df104,df105,df106,df107,df108,df109,df110,df111,df112,df113,df114,df115,df116,df117,df118,df119,\n",
    "              df120,df121,df122,df123,df124,df125,df126,df127,df128,df129,df130,df131,df132,df133,df134,df135,df136,df137,df138,\n",
    "              df139,df140,df141,df142,df143,df144,df145,df146,df147,df148,df149,df150,df151,df152,df153,df154,df155,df156,df157,\n",
    "              df158,df159,df160,df161,df162,df163,df164,df165,df166,df167,df168,df169,df170,df171,df172,df173,df174,df175,df176,\n",
    "              df177,df178,df179,df180,df181,df182,df183,df184,df185,df186,df187,df188,df189,df190,df191,df192,df193,df194,df195,\n",
    "              df196,df197,df198,df199,df200\n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "2c3dc3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset=df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "a6d86476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Vertical</th>\n",
       "      <th>Healines</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monday, 31 July 2017</th>\n",
       "      <td>Pioneer</td>\n",
       "      <td>HOME\\nCOLUMNISTS\\nEDIT</td>\n",
       "      <td>Democracy in danger</td>\n",
       "      <td>With Nawaz Sharif gone, Pakistan Army will be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monday, 31 July 2017</th>\n",
       "      <td>Pioneer</td>\n",
       "      <td>HOME\\nCOLUMNISTS\\nEDIT</td>\n",
       "      <td>Unshackling IIMs, others</td>\n",
       "      <td>Centres desire to give greater autonomy to hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monday, 31 July 2017</th>\n",
       "      <td>Thomas Mathew Kadavil</td>\n",
       "      <td>HOME\\nCOLUMNISTS\\nOPED</td>\n",
       "      <td>Setting norms for migration</td>\n",
       "      <td>To change the narrative and perceptions on mig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monday, 31 July 2017</th>\n",
       "      <td>Vinayshil Gautam</td>\n",
       "      <td>HOME\\nCOLUMNISTS\\nOPED</td>\n",
       "      <td>Age of superficial fact-finding</td>\n",
       "      <td>In an era of ostentatious presentations, one s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monday, 31 July 2017</th>\n",
       "      <td>Annpurna Nautiyal</td>\n",
       "      <td>HOME\\nCOLUMNISTS\\nOPED</td>\n",
       "      <td>Modi's foreign policy visits can bear fruit</td>\n",
       "      <td>Indias confusion regarding its allies has alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saturday, 01 July 2017</th>\n",
       "      <td>IANS</td>\n",
       "      <td>HOME\\nSPORTS BYTES</td>\n",
       "      <td>Athawale accuses Kohli, others of 'fixing' ICC...</td>\n",
       "      <td>Union Minister Ramdas Athawale has accused Ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saturday, 01 July 2017</th>\n",
       "      <td>PTI</td>\n",
       "      <td>HOME\\nSPORTS BYTES</td>\n",
       "      <td>We've a good chance of winning medal at World ...</td>\n",
       "      <td>Indian shuttlers have a great chance of winnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saturday, 01 July 2017</th>\n",
       "      <td>PNS</td>\n",
       "      <td>HOME\\nPAGE 1</td>\n",
       "      <td>Cheers for farmers: Tax on fertilizer cut</td>\n",
       "      <td>Now, farmers have enough reasons to cheer as t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saturday, 01 July 2017</th>\n",
       "      <td>IANS</td>\n",
       "      <td>HOME\\nPOTPOURRI</td>\n",
       "      <td>Khushi didn't participate in any dance show: S...</td>\n",
       "      <td>Sridevi's younger daughter Khushi has been in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saturday, 01 July 2017</th>\n",
       "      <td>IANS</td>\n",
       "      <td>HOME\\nPOTPOURRI</td>\n",
       "      <td>Rakesh Roshan completes 50 years in showbiz, H...</td>\n",
       "      <td>Veteran actor-filmmaker Rakesh Roshan has comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Author                Vertical  \\\n",
       "Date                                                                    \n",
       "Monday, 31 July 2017                  Pioneer  HOME\\nCOLUMNISTS\\nEDIT   \n",
       "Monday, 31 July 2017                  Pioneer  HOME\\nCOLUMNISTS\\nEDIT   \n",
       "Monday, 31 July 2017    Thomas Mathew Kadavil  HOME\\nCOLUMNISTS\\nOPED   \n",
       "Monday, 31 July 2017         Vinayshil Gautam  HOME\\nCOLUMNISTS\\nOPED   \n",
       "Monday, 31 July 2017        Annpurna Nautiyal  HOME\\nCOLUMNISTS\\nOPED   \n",
       "...                                       ...                     ...   \n",
       "Saturday, 01 July 2017                   IANS      HOME\\nSPORTS BYTES   \n",
       "Saturday, 01 July 2017                    PTI      HOME\\nSPORTS BYTES   \n",
       "Saturday, 01 July 2017                    PNS            HOME\\nPAGE 1   \n",
       "Saturday, 01 July 2017                   IANS         HOME\\nPOTPOURRI   \n",
       "Saturday, 01 July 2017                   IANS         HOME\\nPOTPOURRI   \n",
       "\n",
       "                                                                 Healines  \\\n",
       "Date                                                                        \n",
       "Monday, 31 July 2017                                  Democracy in danger   \n",
       "Monday, 31 July 2017                             Unshackling IIMs, others   \n",
       "Monday, 31 July 2017                          Setting norms for migration   \n",
       "Monday, 31 July 2017                      Age of superficial fact-finding   \n",
       "Monday, 31 July 2017          Modi's foreign policy visits can bear fruit   \n",
       "...                                                                   ...   \n",
       "Saturday, 01 July 2017  Athawale accuses Kohli, others of 'fixing' ICC...   \n",
       "Saturday, 01 July 2017  We've a good chance of winning medal at World ...   \n",
       "Saturday, 01 July 2017          Cheers for farmers: Tax on fertilizer cut   \n",
       "Saturday, 01 July 2017  Khushi didn't participate in any dance show: S...   \n",
       "Saturday, 01 July 2017  Rakesh Roshan completes 50 years in showbiz, H...   \n",
       "\n",
       "                                                              Description  \n",
       "Date                                                                       \n",
       "Monday, 31 July 2017    With Nawaz Sharif gone, Pakistan Army will be ...  \n",
       "Monday, 31 July 2017    Centres desire to give greater autonomy to hi...  \n",
       "Monday, 31 July 2017    To change the narrative and perceptions on mig...  \n",
       "Monday, 31 July 2017    In an era of ostentatious presentations, one s...  \n",
       "Monday, 31 July 2017    Indias confusion regarding its allies has alr...  \n",
       "...                                                                   ...  \n",
       "Saturday, 01 July 2017  Union Minister Ramdas Athawale has accused Ind...  \n",
       "Saturday, 01 July 2017  Indian shuttlers have a great chance of winnin...  \n",
       "Saturday, 01 July 2017  Now, farmers have enough reasons to cheer as t...  \n",
       "Saturday, 01 July 2017  Sridevi's younger daughter Khushi has been in ...  \n",
       "Saturday, 01 July 2017  Veteran actor-filmmaker Rakesh Roshan has comp...  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "7a2998a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving into csv\n",
    "df_reset.to_csv('the_pioneer_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2df4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04465cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
