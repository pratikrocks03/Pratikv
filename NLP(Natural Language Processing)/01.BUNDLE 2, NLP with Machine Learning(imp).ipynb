{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e669067d",
   "metadata": {},
   "source": [
    "# NLTK(natural language tool kit) WORD, SENTENCE and REGULAR TOKENIZATION( First week)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd80d6",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is aprocess of breaking down a given paragraph of text into a list of sentence or words. When paragraph is broken down into list of sentences, It is called sentence tokenization. Similarly, if the sentences are further broken down into list of words, it is known as word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d03044b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda2\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda2\\lib\\site-packages (from nltk) (8.0.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda2\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda2\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda2\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda2\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# installing nltk\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaea5cc",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5d8c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e5c98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deinining the text\n",
    "text= 'I am learning Natural Language Processing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3643b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Natural', 'Language', 'Processing']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b7b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ee026f9",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b0d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65b43f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definining Text\n",
    "text='Our Company annual growth rate is 25.50%. Good job Mr. Bajaj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe634047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our Company annual growth rate is 25.50%.', 'Good job Mr. Bajaj']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92179ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "737e686d",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "791e0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0391b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "text=\"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have session at 11AM!. We can learn a lot of $.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d5c528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'an',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'texts',\n",
       " 'and',\n",
       " 'sounds',\n",
       " 'but',\n",
       " 'can',\n",
       " 't',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'images',\n",
       " 'e',\n",
       " 'have',\n",
       " 'session',\n",
       " 'at',\n",
       " 'e',\n",
       " 'can',\n",
       " 'learn',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print woed by woed that contains all small case and starts from small a to z\n",
    "regexp_tokenize(text,\"[a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1c30bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'an',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'texts',\n",
       " 'and',\n",
       " 'sounds',\n",
       " 'but',\n",
       " \"can't\",\n",
       " 'deal',\n",
       " 'with',\n",
       " 'images',\n",
       " 'e',\n",
       " 'have',\n",
       " 'session',\n",
       " 'at',\n",
       " 'e',\n",
       " 'can',\n",
       " 'learn',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print extra quote 'get's you word like cant't , don,t\n",
    "regexp_tokenize(text,\"['a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fd2f199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', 'C', 'W', 'AM', 'W']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print word by word that contains all caps and from caps A to Z\n",
    "regexp_tokenize(text,\"[A-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56aedf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have session at 11AM!. We can learn a lot of $.\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print everything in one line\n",
    "regexp_tokenize(text,\"[\\a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c7663da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' C',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ', ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '. W',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' 11AM!. W',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' $.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print Anything starts with caret is not equal\n",
    "regexp_tokenize(text,\"[^a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "963a5f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print only numbers\n",
    "regexp_tokenize(text,\"[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32624897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have session at \",\n",
       " 'AM!. We can learn a lot of $.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print without numbers\n",
    "regexp_tokenize(text,\"[^0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43bbbe21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(text,\"[$]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19437a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd87330e",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "stop words are such words which are very common in occurence such as 'a', 'an', 'the', 'at', etc. We ignore such words during the preprocessing part since they do not give any important information and would just take additional space. We can make our custom list of stop words as well if we want. Different libraries have diffrent stop words list. Let's see the stop words list for NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8db026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# If getting error download stopwords as below\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7321be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706401ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# printing default stopwords\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245fcd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking length of default stopwords\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f700be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way \n",
    "import nltk\n",
    "stopset=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e6f3658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afc84bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adiing customize stopwords\n",
    "stopset.update(('new','old'))\n",
    "# two now two customize stopwords will be added\n",
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25dddd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'new',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bc09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2318a30",
   "metadata": {},
   "source": [
    "#  similar to the stopwrds, we can also ignore punctuations in our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b5b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a797d276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation # punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a426df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and Punctuations from the above set or texts\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "punct=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "418f8f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking stopwords\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b901f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c21475bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length ==> 161\n",
      "length of cleaned text ==> 16\n",
      "\n",
      " ['India', 'Hindi', 'Bharat', 'officially', 'Republic', 'India', 'country', 'South', 'Asia', 'It', 'seventh-largest', 'country', 'area', 'second-most', 'populous', 'country']\n"
     ]
    }
   ],
   "source": [
    "# our text\n",
    "text=\"India (Hindi: Bharat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country.\"\n",
    "\n",
    "# Empty List to Load clean data\n",
    "cleaned_text=[]\n",
    "\n",
    "for word in nltk.word_tokenize(text):\n",
    "    if word not in punct:\n",
    "        if word not in stop_words:\n",
    "            cleaned_text.append(word)\n",
    "            \n",
    "            \n",
    "print('Original length ==>', len(text))\n",
    "print ('length of cleaned text ==>', len(cleaned_text))\n",
    "print('\\n', cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039526b",
   "metadata": {},
   "source": [
    "# Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a52ef7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india (hindi: bharat), officially the republic of india, is a country in south asia. it is the seventh-largest country by area, the second-most populous country.\n",
      "\n",
      " INDIA (HINDI: BHARAT), OFFICIALLY THE REPUBLIC OF INDIA, IS A COUNTRY IN SOUTH ASIA. IT IS THE SEVENTH-LARGEST COUNTRY BY AREA, THE SECOND-MOST POPULOUS COUNTRY.\n"
     ]
    }
   ],
   "source": [
    "# convert into lower case\n",
    "print (text.lower())\n",
    "\n",
    "#convert into upper case\n",
    "print ('\\n',text.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8b7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add1673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa142da",
   "metadata": {},
   "source": [
    "# Stemming\n",
    " > Stemming means mapping a group of words to the same stem by removing prefixes or suffixes without giving any value to the \"grammatical meaning\" of the stem formed after the process.\n",
    " \n",
    "e.g\n",
    "\n",
    "computation--> comput\n",
    "\n",
    "computer--> comput\n",
    "\n",
    "hobbies--> hobbi\n",
    "\n",
    "We can see that stemming tries to bring the word back to their base word but the base word may or may not have correct grammatical meanings.\n",
    "\n",
    "There are few types of stemmers available in NLTK package. We will talk about popular two\n",
    "\n",
    "1) porter Stemmer\n",
    "\n",
    "2) Lancaster Stemmer\n",
    "\n",
    "Lets see how to use both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e736512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df92003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "***************************\n",
      "Lancaster Stemmer\n",
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "lancaster= LancasterStemmer()\n",
    "porter= PorterStemmer()\n",
    "snowball= SnowballStemmer('english')\n",
    "\n",
    "#example\n",
    "print('Porter Stemmer')\n",
    "print(porter.stem('hobby'))\n",
    "print(porter.stem('hobbies'))\n",
    "print(porter.stem('computer'))\n",
    "print(porter.stem('computation'))\n",
    "print('***************************')\n",
    "\n",
    "print('Lancaster Stemmer')\n",
    "print(lancaster.stem('hobby'))\n",
    "print(lancaster.stem('hobbies'))\n",
    "print(lancaster.stem('computer'))\n",
    "print(lancaster.stem('computation'))\n",
    "\n",
    "#see the differnce between both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1749a096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'the',\n",
       " 'office',\n",
       " 'on',\n",
       " 'my',\n",
       " 'bike',\n",
       " 'when',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'car',\n",
       " 'passing',\n",
       " 'by',\n",
       " 'hit',\n",
       " 'the',\n",
       " 'tree',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see with a new sentence\n",
    "sentence=\"I was going to the office on my bike when i saw a car passing by hit the tree.\"\n",
    "\n",
    "token=list(nltk.word_tokenize(sentence))\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a83c2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was going to the office on my bike when i saw a car passing by hit the tree.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "054370ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was go to the offic on my bike when i saw a car pass by hit the tree . \n",
      "\n",
      "i was going to the off on my bik when i saw a car pass by hit the tre . \n",
      "\n",
      "i wa go to the offic on my bike when i saw a car pass by hit the tree . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for stemmer in (snowball ,lancaster,porter):\n",
    "    stemm=[stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm),'\\n')\n",
    "    \n",
    "# lancaster algorithm is faster than porter but it is more complex.Porter stemmer is the oldest algorithm present and was the most popular to use\n",
    "\n",
    "#Snowball stemmer is also known as porter2, it is the updated version of the porter stemmer and is currently the most popular stemming algorith.\n",
    "\n",
    "#Snowball stemmer is available for multiple languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f527a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "# one more simple example of porter\n",
    "print(porter.stem('running'))\n",
    "print(porter.stem('runs'))\n",
    "print(porter.stem('ran'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6ae1f",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmitization also does the same thing as stemming and try to bring a word to its base form, but unlike stemming it do keep in account the actual meaning of the base word i.e. the base word belongs to any specific language. The 'base word' is known as 'Lemma'\n",
    "\n",
    "We use WordNet Lemmatizer for Lemmatization in nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b330a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f068738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "lemma =WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize(\"running\"))\n",
    "print(lemma.lemmatize(\"runs\"))\n",
    "print(lemma.lemmatize(\"ran\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bedc37",
   "metadata": {},
   "source": [
    "Here, We can see the lemma has chaanged for the words with same base.\n",
    "\n",
    "This is because, we haven't given any context to the Lemmatizer.\n",
    "\n",
    "Generally, it is given by passing the POS9part of speech) tags for the words in a sentence. e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "214ba80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('running',pos='v'))\n",
    "print(lemma.lemmatize('ran',pos='v'))\n",
    "print(lemma.lemmatize('ran',pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648d671",
   "metadata": {},
   "source": [
    "Lemmatizer is ver complex and takes a lot of time to calculate.\n",
    "\n",
    "So, it should only when the real meaning of words or the context is necessary for processing, else stemming should be preferred.\n",
    "\n",
    "It completely depends on the type of problem you are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "079e965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for Bring is bring\n",
      "Stemming for King is king\n",
      "Stemming for Going is go\n",
      "Stemming for Anything is anyth\n",
      "Stemming for Sing is sing\n",
      "Stemming for Ring is ring\n",
      "Stemming for Nothing is noth\n",
      "Stemming for Thing is thing\n"
     ]
    }
   ],
   "source": [
    "# one more example using both stemming and Lemma\n",
    "text=\"Bring King Going Anything Sing Ring Nothing Thing\"\n",
    "\n",
    "# stemming\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer=PorterStemmer()\n",
    "\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w, porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45cff5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Bring is Bring\n",
      "Lemma for King is King\n",
      "Lemma for Going is Going\n",
      "Lemma for Anything is Anything\n",
      "Lemma for Sing is Sing\n",
      "Lemma for Ring is Ring\n",
      "Lemma for Nothing is Nothing\n",
      "Lemma for Thing is Thing\n"
     ]
    }
   ],
   "source": [
    "# NOw see same thing with Lemma, it will give some meaning of the text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer =WordNetLemmatizer()\n",
    "\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b677827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b68a0be7",
   "metadata": {},
   "source": [
    "# Wordnet\n",
    "> Wordnet is NLTK corpus reader, a lexical database for English, It can be used to find the meaning of words, synonym or antonyym . One can define it as a sematically oriented dictionary of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "017db4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "103af886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms => {'fighting', 'alive', 'active_voice', 'participating', 'active', 'active_agent', 'dynamic', 'combat-ready'}\n",
      "Antonyms => {'passive_voice', 'extinct', 'dormant', 'inactive', 'quiet', 'passive', 'stative'}\n"
     ]
    }
   ],
   "source": [
    "# Lets find synonyms and antonyms using python code\n",
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(\"active\"):  # active is random word\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print('synonyms =>',set(synonyms))\n",
    "print('Antonyms =>',set(antonyms))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d0170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb6a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187f8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872d4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "109f9951",
   "metadata": {},
   "source": [
    "# Word Embedding (second week)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e96ad2",
   "metadata": {},
   "source": [
    "# Count Vectorizer\n",
    "count vectorizer uses  two of the following models as the base to vectorize the given words on the basis of frquency of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be664a4b",
   "metadata": {},
   "source": [
    "## 1) Bag of words Model\n",
    "This model is used in NLP to represent the given/text/sentence/document as a collection(bag) of words without giving any importance to grammar or the occurence order of the words. It keeps the account of frequency of the words in the text document , which can be used as features in many models.\n",
    "> It is manily used for feature selection.\n",
    "> Stop words are the most frequent words that appears in raw document. Thus, having a word with high frequency count doesn't mean that he word is as important. to resolve this problem ,\"Tf-idf was introduced. We will discuss about it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6d07f",
   "metadata": {},
   "source": [
    "## 2) n-gram model\n",
    "As discussed in bag of words model.BOW model doesn't keep the sequence of words in a given text, only the frequency of words matters.\n",
    "- n-gram model is used in cases to keep the context of the given  text intact.\n",
    "- N-gram is the sequence of n words from agiven text/document.\n",
    "\n",
    "When,\n",
    "n=1(unigram)\n",
    "\n",
    "n=2(bigram)\n",
    "\n",
    "n=3(trigram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e43b9",
   "metadata": {},
   "source": [
    "## 3) Skip grams\n",
    "It is type of n-grams whwre the words are not necessarily in the same order as are in the text. i.e. some words can be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c296249",
   "metadata": {},
   "source": [
    "##  Lets see the implementation of count vectorizer in python:\n",
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b0f662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "# Example of single document\n",
    "# without stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# Single document(',' seprates each document)\n",
    "string=['This is an example of bag of words']\n",
    "\n",
    "# This step will convert text into tokens\n",
    "vect1= CountVectorizer()\n",
    "\n",
    "vect1.fit_transform(string)\n",
    "print(\"bag of words :\", vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a10188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'an': 0, 'example': 2, 'of': 4, 'bag': 1, 'words': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting index\n",
    "vect1.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfea01f",
   "metadata": {},
   "source": [
    "> Fit and transform and predict if the word is present or not\n",
    "\n",
    "> This is widely used for document or subject classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560feec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process\n",
    "c_vect = CountVectorizer()\n",
    "c_vect.fit(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0df604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Present at [[0 0 0 2 1 0 0]]\n",
      "original indexes ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "string2=['Lets understand is of word is']\n",
    "\n",
    "c_new_vect=c_vect.transform(string2)\n",
    "\n",
    "print(\"Text Present at\",c_new_vect.toarray())\n",
    "\n",
    "# compare with the indexes \n",
    "print(\"original indexes\", vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef250ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...])\n"
     ]
    }
   ],
   "source": [
    "## Bag of words using stopwords (you can avoid writing extra steps to remove stopwords)\n",
    "stpwords=stopwords.words('english')\n",
    "\n",
    "string=['This is an example of bag of words!']\n",
    "vect1=CountVectorizer(stop_words=stpwords)\n",
    "print(vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d58acc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['bag', 'example', 'words']\n",
      "vocab        : {'example': 1, 'bag': 0, 'words': 2}\n"
     ]
    }
   ],
   "source": [
    "# this process will not show the stop words\n",
    "vect1.fit_transform(string)\n",
    "\n",
    "print(\"bag of words :\", vect1.get_feature_names())\n",
    "print(\"vocab        :\",vect1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e181863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using function\n",
    "def text_matrix(message,countvect):\n",
    "    terms_doc=countvect.fit_transform(message)\n",
    "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fddb2f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below metrix is the Bag of Words approach\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>but</th>\n",
       "      <th>for</th>\n",
       "      <th>get</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>making</th>\n",
       "      <th>mantra</th>\n",
       "      <th>natural</th>\n",
       "      <th>only</th>\n",
       "      <th>practice</th>\n",
       "      <th>processing</th>\n",
       "      <th>progress</th>\n",
       "      <th>slowly</th>\n",
       "      <th>success</th>\n",
       "      <th>the</th>\n",
       "      <th>there</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  but  for  get  in  is  language  making  mantra  natural  only  \\\n",
       "0    1    0    0    0   1   0         1       1       0        1     0   \n",
       "1    0    0    0    1   0   0         0       0       0        0     0   \n",
       "2    0    1    1    0   0   1         0       0       1        0     1   \n",
       "\n",
       "   practice  processing  progress  slowly  success  the  there  we  will  \n",
       "0         0           1         1       2        0    0      0   1     0  \n",
       "1         0           0         0       0        0    0      1   1     1  \n",
       "2         1           0         0       0        1    1      0   0     0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message=['We are slowly slowly making progress in Natural Language Processing',\n",
    "        \"We will get there\",\"But practice is the only mantra for success\"]\n",
    "\n",
    "c_vect=CountVectorizer()\n",
    "print(\"Below metrix is the Bag of Words approach\")\n",
    "text_matrix(message, c_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289af64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40d02321",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "974107f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: ['an', 'exapmle', 'gram', 'is', 'of', 'this']\n",
      "2-gram: ['an exapmle', 'exapmle of', 'is an', 'of gram', 'this is']\n",
      "3-gram: ['an exapmle of', 'exapmle of gram', 'is an exapmle', 'this is an']\n",
      "4-gram ['an exapmle of gram', 'is an exapmle of', 'this is an exapmle']\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string=[\"This is an exapmle of gram!\"]\n",
    "\n",
    "#unigram\n",
    "vect1=CountVectorizer(ngram_range=(1,1))\n",
    "vect1.fit_transform(string)\n",
    "\n",
    "# Bigram\n",
    "vect2=CountVectorizer(ngram_range=(2,2))\n",
    "vect2.fit_transform(string)\n",
    "\n",
    "#Trigram\n",
    "vect3=CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(string)\n",
    "\n",
    "#4gram\n",
    "vect4=CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(string)\n",
    "\n",
    "\n",
    "print(\"1-gram:\",vect1.get_feature_names())\n",
    "print(\"2-gram:\",vect2.get_feature_names())\n",
    "print(\"3-gram:\",vect3.get_feature_names())\n",
    "print(\"4-gram\",vect4.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7ee81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8606eab0",
   "metadata": {},
   "source": [
    "# Tf-Idf (Term frequency-Inverse document frequency)\n",
    "Tf-Idf, short for term frequency-inverse document frquency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The Tf-Idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documments in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf-Idf is one of ghe most popular term-weighting schemes today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905046b3",
   "metadata": {},
   "source": [
    "## Term Frequency\n",
    "It is simply the frequency in which a word appears in a document in comparison to the total number words in the document. Mathematically given as:\n",
    "\n",
    "Term frequency=(Number of times a word appears in the document)/ Total number of words in the document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36ee67",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency\n",
    "Term frequency has disadvantage that it tends to give higher weights to words with higher frequency. In such cases words like 'a', 'the', 'in', 'of' etc. appears more in the docuents than other regular words. Thus, more important words are wrongly given lower weights as their frequency is less. To tackle this problem IDF was introduced. IDF decreases the weights of such high frequency terms and increases the weight of terms with rare occurence. Mathematically It is given as:\n",
    "\n",
    "Inverse Document Frequency=log[(Number of documents)/(Number of documents the word appears in)]\n",
    "\n",
    "> note : log has base 2\n",
    "\n",
    ">Fianlly : TF-IDF score = Term frequency * Inverse Document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d02bd0",
   "metadata": {},
   "source": [
    "Lets see python implementation for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52298ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    an   be  can  confusing  example       how  idf   is        it       see  \\\n",
      "0  0.5  0.0  0.0        0.0      0.5  0.000000  0.0  0.5  0.000000  0.000000   \n",
      "1  0.0  0.0  0.0        0.0      0.0  0.408248  0.0  0.0  0.408248  0.408248   \n",
      "2  0.0  0.5  0.5        0.5      0.0  0.000000  0.5  0.0  0.000000  0.000000   \n",
      "\n",
      "   this        we      will     works  \n",
      "0   0.5  0.000000  0.000000  0.000000  \n",
      "1   0.0  0.408248  0.408248  0.408248  \n",
      "2   0.0  0.000000  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfid=TfidfVectorizer(smooth_idf=False)\n",
    "\n",
    "doc=[\"This is an example.\",\"We will see how it works.\",\"IDF can be confusing\"] #3 documents\n",
    "\n",
    "doc_vector=tfid.fit_transform(doc)\n",
    "\n",
    "#print(tfid.get_feature_names())\n",
    "\n",
    "df=pd.DataFrame(doc_vector.todense(), columns=tfid.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cfc2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using function\n",
    "def text_matrix(message, countvect):\n",
    "    term_doc=countvect.fit_transform(message)\n",
    "    return pd.DataFrame(term_doc.toarray(),columns=countvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8d0aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>cases</th>\n",
       "      <th>covid</th>\n",
       "      <th>dropping</th>\n",
       "      <th>is</th>\n",
       "      <th>nothing</th>\n",
       "      <th>that</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.501651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.322745</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        are     cases     covid  dropping        is   nothing      that  \\\n",
       "0  0.000000  0.000000  0.592567  0.000000  0.381519  0.000000  0.501651   \n",
       "1  0.000000  0.000000  0.425441  0.000000  0.547832  0.720333  0.000000   \n",
       "2  0.546454  0.546454  0.322745  0.546454  0.000000  0.000000  0.000000   \n",
       "\n",
       "       what  \n",
       "0  0.501651  \n",
       "1  0.000000  \n",
       "2  0.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will call the function created earlier\n",
    "feb_message=[\"What is that covid covid\",\n",
    "            \"covid is nothing\",\n",
    "            \"covid cases are dropping\"]\n",
    "\n",
    "tf=TfidfVectorizer()\n",
    "\n",
    "# passing same message with TF-IDF\n",
    "\n",
    "text_matrix(feb_message,tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09cde0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>covid</th>\n",
       "      <th>is</th>\n",
       "      <th>that</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668501</td>\n",
       "      <td>0.334251</td>\n",
       "      <td>0.469778</td>\n",
       "      <td>0.469778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704909</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bad     covid        is      that      what\n",
       "0  0.000000  0.668501  0.334251  0.469778  0.469778\n",
       "1  0.704909  0.501549  0.501549  0.000000  0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importance of covid increased based on the occurence and total document\n",
    "jul_message=[\"What is that covid covid\",\n",
    "            \"covid is bad\"]\n",
    "\n",
    "text_matrix(jul_message,tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdd8b5",
   "metadata": {},
   "source": [
    "## CountVectorizer, TF_IDF, n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad29a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "\n",
    "arr=[\"Car was cleaned by Jack\",\n",
    "    \"Jack was cleaned by Car\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "846a1eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names \n",
      " ['by car', 'by jack', 'car was', 'cleaned by', 'jack was', 'was cleaned']\n",
      "Array \n",
      " [[0 1 1 1 0 1]\n",
      " [1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# If you want to take into account just term frequencies:\n",
    "vectorizer=CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# The ngram range specifies your ngram configuration.\n",
    "\n",
    "X=vectorizer.fit_transform(arr)\n",
    "\n",
    "# Testing the ngram generation:\n",
    "print(\"Feature Names \\n\", vectorizer.get_feature_names())\n",
    "\n",
    "print('Array \\n', X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ee87f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.57615236 0.57615236 0.40993715 0.         0.40993715]\n",
      " [0.57615236 0.         0.         0.40993715 0.57615236 0.40993715]]\n"
     ]
    }
   ],
   "source": [
    "# And now testing TFIDF vectorizer:\n",
    "# You can still specify n-grams here.\n",
    "\n",
    "vectorizer=TfidfVectorizer(ngram_range=(2,2))\n",
    "X=vectorizer.fit_transform(arr)\n",
    "\n",
    "# Testinng the TFIDF value + ngrams:\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b4c8daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.40546511 1.40546511 1.         0.         1.        ]\n",
      " [1.40546511 0.         0.         1.         1.40546511 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing TFIDF vectorizer without normalization:\n",
    "# You can still specify n-grams here\n",
    "\n",
    "vectorizer=TfidfVectorizer(ngram_range=(2,2), norm=None)\n",
    "\n",
    "X=vectorizer.fit_transform(arr)\n",
    "\n",
    "# Testing TFIDF value before normalization:\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbe52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99cb0e2b",
   "metadata": {},
   "source": [
    "# Word Emnedding Contd...\n",
    "Example-1. Have a good day and 3. Have a great day\n",
    "\n",
    "Lets consider one hot encoder vector for eaxch of these in V.Length of our one-hot enocded vector would be equal to the size of V(=5. We would have a vector of zeros except  for the element at the index representing the corresponding word in the vocabulary. That particular element would be one.\n",
    "\n",
    "- Lets see how it looks\n",
    "\n",
    "Have=[1,0,0,0,0]; a=[0,1,0,0,0]; good=[0,0,1,0,0]; great=[0,0,0,1,0]; day=[0,0,0,0,1] (represents transpose) If we try to visualize these encodings, we can think of a 5 dimensional space, where each word can occupies one of the dimensions and has nothing to do with the rest (no projrction along the other dimensions). This means 'good' and 'great' are as different as 'day' and 'have', which is not true. Our objective is to have words with similar context occcupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0.\n",
    "\n",
    "Since there are feqw shortcomings in CountVectorizer and TF-IDF vectorizer and those are solved by Word2Vec.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32305a",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a4b13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29edb618",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Today were announcing 135 Crore INR ($18 million USD) in new funding for India. This includes two grants from Google.org, Googles philanthropic arm, totalling 20 Crore INR ($2.6 million USD). The first is to GiveIndia to provide cash assistance to families hit hardest by the crisis to help with their everyday expenses. The second will go to UNICEF to help get urgent medical supplies, including oxygen and testing equipment, to where its needed most in India. It also includes donations from our ongoing employee giving campaign  so far more than 900 Googlers have contributed 3.7 Crore INR ($500,000 USD) for organizations supporting high-risk and marginalized communities.' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ab2e787",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paragraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-ed463405db88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# preprocessing the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparagraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\[[0-9]*\\]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\s+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'paragraph' is not defined"
     ]
    }
   ],
   "source": [
    "# preprocessing the data\n",
    "paragraph=paragraph.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "text=re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "\n",
    "text=text.lower()\n",
    "\n",
    "text=re.sub(r'\\d',' ',text)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4d9c691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today were announcing crore inr million usd in new funding for india this includes two grants from googleorg googles philanthropic arm totalling crore inr million usd the first is to giveindia to provide cash assistance to families hit hardest by the crisis to help with their everyday expenses the second will go to unicef to help get urgent medical supplies including oxygen and testing equipment to where its needed most in india it also includes donations from our ongoing employee giving campaign  so far more than googlers have contributed crore inr usd for organizations supporting highrisk and marginalized communities']\n"
     ]
    }
   ],
   "source": [
    "# preparing the dataset\n",
    "sentences =nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a419631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['today', 'we', '', 're', 'announcing', 'crore', 'inr', 'million', 'usd', 'in', 'new', 'funding', 'for', 'india', 'this', 'includes', 'two', 'grants', 'from', 'googleorg', 'google', '', 's', 'philanthropic', 'arm', 'totalling', 'crore', 'inr', 'million', 'usd', 'the', 'first', 'is', 'to', 'giveindia', 'to', 'provide', 'cash', 'assistance', 'to', 'families', 'hit', 'hardest', 'by', 'the', 'crisis', 'to', 'help', 'with', 'their', 'everyday', 'expenses', 'the', 'second', 'will', 'go', 'to', 'unicef', 'to', 'help', 'get', 'urgent', 'medical', 'supplies', 'including', 'oxygen', 'and', 'testing', 'equipment', 'to', 'where', 'it', '', 's', 'needed', 'most', 'in', 'india', 'it', 'also', 'includes', 'donations', 'from', 'our', 'ongoing', 'employee', 'giving', 'campaign', '', 'so', 'far', 'more', 'than', 'googlers', 'have', 'contributed', 'crore', 'inr', 'usd', 'for', 'organizations', 'supporting', 'highrisk', 'and', 'marginalized', 'communities']]\n"
     ]
    }
   ],
   "source": [
    "sent_word=[nltk.word_tokenize(sentence)for sentence in sentences]\n",
    "print(sent_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62d8da14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce5ce6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_word=[nltk.word_tokenize(sentence)for sentence in sentences]\n",
    "for i in range(len(sent_word)):\n",
    "    sent_word[i]=[word for word in sent_word[i]\n",
    "                 if word not in stopwords.words('english') if word not in punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ac20c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, 'crore': 1, 'inr': 2, 'usd': 3, 'million': 4, 'help': 5, 'india': 6, 'includes': 7, 'communities': 8, 'cash': 9, 'giveindia': 10, 'provide': 11, 'families': 12, 'assistance': 13, 'totalling': 14, 'hit': 15, 'hardest': 16, 'first': 17, 'googleorg': 18, 'arm': 19, 'philanthropic': 20, 'google': 21, 'grants': 22, 'two': 23, 'funding': 24, 'new': 25, 'announcing': 26, 'crisis': 27, 'everyday': 28, 'marginalized': 29, 'expenses': 30, 'highrisk': 31, 'supporting': 32, 'organizations': 33, 'contributed': 34, 'googlers': 35, 'far': 36, '': 37, 'campaign': 38, 'giving': 39, 'employee': 40, 'ongoing': 41, 'donations': 42, 'also': 43, 'needed': 44, 'equipment': 45, 'testing': 46, 'oxygen': 47, 'including': 48, 'supplies': 49, 'medical': 50, 'urgent': 51, 'get': 52, 'unicef': 53, 'go': 54, 'second': 55, 'today': 56}\n"
     ]
    }
   ],
   "source": [
    "# Training the Word2Vec\n",
    "model=Word2Vec(sent_word, min_count=1)\n",
    "\n",
    "words=model.wv.key_to_index\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59d95bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00256086  0.00086745 -0.00255346  0.00934183  0.00276054  0.00408132\n",
      " -0.0011714   0.00092609  0.00661568 -0.00073138  0.00333565 -0.00069001\n",
      "  0.00524941  0.00363087  0.00257752 -0.00534389 -0.00467956  0.00430685\n",
      " -0.00591499 -0.00019274 -0.00063853  0.00347008 -0.0084208   0.00879213\n",
      " -0.00146304 -0.00534656  0.00404576 -0.00194934 -0.00777336 -0.00449529\n",
      " -0.00034076 -0.008978    0.00056361  0.00240192 -0.00321757  0.00256867\n",
      "  0.00248598  0.01000121  0.00144347  0.00200473  0.0027967  -0.00209657\n",
      " -0.00870643  0.00801435 -0.00198888 -0.00969615 -0.00655285 -0.00395269\n",
      "  0.00395991  0.00504422  0.00610003 -0.00680153  0.00068832 -0.00275246\n",
      " -0.00523002  0.00699061  0.00399895 -0.00311523 -0.00830512 -0.00513452\n",
      " -0.00063819  0.00780149  0.00607099 -0.00845046 -0.00955539  0.00717185\n",
      " -0.00233412 -0.00369468  0.00571558 -0.0058518   0.00508982 -0.00023902\n",
      " -0.00682946 -0.00031798  0.00638102  0.00928811  0.00220864  0.00506217\n",
      " -0.00499239 -0.00081104 -0.00534962  0.00118982 -0.00181079 -0.00363204\n",
      " -0.00703466  0.00962154  0.00297814 -0.0022808  -0.00417188  0.00771175\n",
      " -0.0064591   0.00313542  0.00078561  0.00830555  0.0068608  -0.00289921\n",
      "  0.00252921 -0.00166937 -0.00947171 -0.00263215]\n"
     ]
    }
   ],
   "source": [
    "# test the word vectors\n",
    "vector=model.wv['philanthropic']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40a2867d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('expenses', 0.1880984753370285),\n",
       " ('inr', 0.14967605471611023),\n",
       " ('provide', 0.11302445828914642),\n",
       " ('hardest', 0.10766574740409851),\n",
       " ('help', 0.09952039271593094)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar words\n",
    "similar=model.wv.most_similar('philanthropic',topn=5)\n",
    "similar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90fbbb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18809846"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same words similarity\n",
    "model.wv.similarity(w1='philanthropic',w2='expenses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b7dc0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crore'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter on non similarity\n",
    "model.wv.doesnt_match(['million','crore','inr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b196d78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tsne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-7ef4b687fede>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mtsne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tsne' is not defined"
     ]
    }
   ],
   "source": [
    "# cosine\n",
    "from sklearn.manifold import TSNE\n",
    "vocab=['philanthropic','crore','million','expenses','provide','hardest','help']\n",
    "def tsne_plot(model):\n",
    "    labels=[]\n",
    "    wordvecs=[]\n",
    "    \n",
    "    for word in vocab:\n",
    "        wordvecs.append(model[word])\n",
    "        labels.append(word)\n",
    "        \n",
    "        \n",
    "    tsne_model=TSNE(preplexity=3, n_componenets=2, init='pca', random_state=42)\n",
    "    coordinates=tsne_model.fit_transform(wordvecs)\n",
    "    \n",
    "    \n",
    "    x=[]\n",
    "    y=[]\n",
    "    for value in coordinates:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i]),\n",
    "        xy=(x[i],y[i]),\n",
    "        xytext=(2,2),\n",
    "        textcoords=('offset points')\n",
    "                     \n",
    "       \n",
    "    \n",
    "plt.show()\n",
    "tsne.plot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1faf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e02c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f35a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9d3e08f",
   "metadata": {},
   "source": [
    "# Web Scrapping Stock_Price(3rd Week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ac2b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "# requests needed to request urls\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e017dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting url\n",
    "\n",
    "url='https://www.marketwatch.com/investing/Stock/AMZN'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0c7303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping detail\n",
    "price=[]\n",
    "pri=driver.find_elements_by_xpath(\"//bg-quote[@class='value']\")\n",
    "for i in pri:\n",
    "    price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e591844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3,569.50']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50c10be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to capture live price from beautifulsoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def live_price():\n",
    "    res=requests.get(url)\n",
    "    soup=BeautifulSoup(res.text,'html')\n",
    "    price=soup.find_all('bg-quote',{'class':\"value\"})[0].find\n",
    "    return price\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e2a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abfb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c122a597",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "## What is Gensim?\n",
    "- Gensim=\"Generate Similar\" is a popular open source natural language processing(NLP) library used for unsupervised topic modeling.\n",
    "- Let's understand important terms and its meaning.\n",
    "\n",
    "Document: some text\n",
    "\n",
    "Corpus: a collecion of documents\n",
    "\n",
    "Vector: a mathematically representation of a document\n",
    "\n",
    "Model: an algorithm for transforming vectors from one representation to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11246575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document: some text\n",
    "document=\"Human machine interface for lab abc computer applications.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ca1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus: a collecion of documents\n",
    "text_corpus=[\n",
    "    \"Human machine interface for lab abc computer applications \",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45f8417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Vector: a mathematically representation of a document\n",
    "import pprint\n",
    "\n",
    "#  create a set of frequent words\n",
    "stoplist=set('for a of the and to in'.split(' '))\n",
    "\n",
    "#lowercase each document, split it by white space and filter out stopwords\n",
    "texts=[[word for word in document.lower().split() if word not in stoplist]\n",
    "       for document in text_corpus]\n",
    "\n",
    "# count word frequencies       \n",
    "from collections import defaultdict\n",
    "\n",
    "frequency=defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token]+=1\n",
    "        \n",
    "# Only keep that words that appear more than once        \n",
    "processed_corpus=[[token for token in text if frequency[token]>1]for text in texts]   \n",
    "\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d795e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Creating dictionaries , which helps during topic modelling\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary=corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52e9c131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "# vectore representation\n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39795dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9d5c983",
   "metadata": {},
   "source": [
    "Model: an algorithm for transforming vectors from one representation to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0fcfde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_vec [(0, 1), (1, 1)]\n",
      "new_vec2 [(0, 2), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# ALways a make pratice of testing small texts when trying something\n",
    "# In each tuple below 1st oocurence is ID and 2nd occurence is count\n",
    "\n",
    "new_doc=\"Human computer interaction\"\n",
    "new_vec=dictionary.doc2bow(new_doc.lower().split())\n",
    "print('new_vec',new_vec)\n",
    "\n",
    "new_doc2=\"Human computer interaction computer\"\n",
    "new_vec2=dictionary.doc2bow(new_doc2.lower().split())\n",
    "print('new_vec2',new_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae84c0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus=[dictionary.doc2bow(text)for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8506499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models \n",
    "\n",
    "# trainn the model\n",
    "tfidf=models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Trasnform from \"sysem minors\" string\n",
    "word=\"system minors\".lower().split()\n",
    "\n",
    "print(tfidf[dictionary.doc2bow(word)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c8070",
   "metadata": {},
   "source": [
    "### Open Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4c6362c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-7c2dba15b3cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdict_STF\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"sample.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_STF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample.txt'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import os\n",
    "\n",
    "dict_STF=corpora.Dictionary(simple_preprocess(line) for line in open(r\"sample.txt\"))\n",
    "\n",
    "print(dict_STF.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8250b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
